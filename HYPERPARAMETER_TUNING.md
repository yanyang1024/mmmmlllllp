# è¶…å‚æ•°è°ƒä¼˜æŒ‡å—

æœ¬æ–‡æ¡£æä¾›äº†MLPæ•°å€¼é¢„æµ‹æ¨¡å‹çš„ç³»ç»Ÿæ€§è¶…å‚æ•°è°ƒä¼˜æ–¹æ³•ã€å·¥å…·å’Œæœ€ä½³å®è·µã€‚

## ğŸ¯ è¶…å‚æ•°è°ƒä¼˜æ¦‚è¿°

### å…³é”®è¶…å‚æ•°åˆ†ç±»

#### 1. ç½‘ç»œæ¶æ„å‚æ•°
- **hidden_layers**: éšè—å±‚ç»“æ„ `[128, 64, 32]`
- **activation**: æ¿€æ´»å‡½æ•° `relu, tanh, sigmoid, leaky_relu`
- **dropout_rate**: Dropoutæ¯”ç‡ `0.0-0.5`
- **batch_norm**: æ˜¯å¦ä½¿ç”¨æ‰¹æ ‡å‡†åŒ– `true/false`

#### 2. è®­ç»ƒå‚æ•°
- **learning_rate**: å­¦ä¹ ç‡ `1e-5 åˆ° 1e-1`
- **batch_size**: æ‰¹æ¬¡å¤§å° `16, 32, 64, 128, 256`
- **epochs**: è®­ç»ƒè½®æ•° `50-500`
- **weight_decay**: æƒé‡è¡°å‡ `1e-6 åˆ° 1e-2`

#### 3. ä¼˜åŒ–å™¨å‚æ•°
- **optimizer.type**: ä¼˜åŒ–å™¨ç±»å‹ `adam, sgd, rmsprop`
- **optimizer.momentum**: åŠ¨é‡ `0.9, 0.95, 0.99` (ä»…SGD)

#### 4. è°ƒåº¦å™¨å‚æ•°
- **scheduler.type**: å­¦ä¹ ç‡è°ƒåº¦å™¨ `plateau, cosine, step`
- **early_stopping_patience**: æ—©åœè€å¿ƒå€¼ `5-50`

## ğŸ” æ•°æ®ç‰¹æ€§é©±åŠ¨çš„è°ƒä¼˜ç­–ç•¥

### 1. åŸºäºæ•°æ®è§„æ¨¡çš„è°ƒä¼˜

```python
# åˆ›å»ºadaptive_tuning.py
import numpy as np
from typing import Dict, Any, Tuple

class AdaptiveTuner:
    """åŸºäºæ•°æ®ç‰¹æ€§çš„è‡ªé€‚åº”è°ƒä¼˜å™¨"""
    
    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = X
        self.y = y
        self.n_samples, self.n_features = X.shape
        self.n_targets = y.shape[1] if y.ndim > 1 else 1
        
    def get_recommended_config(self) -> Dict[str, Any]:
        """æ ¹æ®æ•°æ®ç‰¹æ€§æ¨èé…ç½®"""
        config = self._get_base_config()
        
        # æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´
        config = self._adjust_for_sample_size(config)
        
        # æ ¹æ®ç‰¹å¾ç»´åº¦è°ƒæ•´
        config = self._adjust_for_feature_dimension(config)
        
        # æ ¹æ®ç›®æ ‡å¤æ‚åº¦è°ƒæ•´
        config = self._adjust_for_target_complexity(config)
        
        return config
    
    def _get_base_config(self) -> Dict[str, Any]:
        """åŸºç¡€é…ç½®"""
        return {
            'model': {
                'hidden_layers': [128, 64, 32],
                'activation': 'relu',
                'dropout_rate': 0.2,
                'batch_norm': False
            },
            'training': {
                'batch_size': 32,
                'epochs': 100,
                'learning_rate': 0.001,
                'weight_decay': 1e-5,
                'early_stopping_patience': 10
            },
            'optimizer': {
                'type': 'adam'
            }
        }
    
    def _adjust_for_sample_size(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´é…ç½®"""
        if self.n_samples < 500:
            # å°æ•°æ®é›†ï¼šç®€å•æ¨¡å‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            config['model']['hidden_layers'] = [64, 32]
            config['model']['dropout_rate'] = 0.1
            config['training']['batch_size'] = min(16, self.n_samples // 4)
            config['training']['learning_rate'] = 0.01
            config['training']['epochs'] = 200
            config['training']['early_stopping_patience'] = 20
            
        elif self.n_samples < 2000:
            # ä¸­ç­‰æ•°æ®é›†ï¼šæ ‡å‡†é…ç½®
            config['training']['batch_size'] = min(32, self.n_samples // 8)
            
        else:
            # å¤§æ•°æ®é›†ï¼šå¤æ‚æ¨¡å‹ï¼Œå¤§æ‰¹æ¬¡
            config['model']['hidden_layers'] = [256, 128, 64, 32]
            config['model']['dropout_rate'] = 0.3
            config['model']['batch_norm'] = True
            config['training']['batch_size'] = min(128, self.n_samples // 16)
            config['training']['learning_rate'] = 0.0001
            config['training']['epochs'] = 50
        
        return config
    
    def _adjust_for_feature_dimension(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®ç‰¹å¾ç»´åº¦è°ƒæ•´é…ç½®"""
        if self.n_features > 100:
            # é«˜ç»´ç‰¹å¾ï¼šéœ€è¦æ›´å¤§çš„ç¬¬ä¸€å±‚
            first_layer_size = min(512, self.n_features * 2)
            config['model']['hidden_layers'][0] = first_layer_size
            config['model']['dropout_rate'] = min(0.5, config['model']['dropout_rate'] + 0.1)
            
        elif self.n_features < 10:
            # ä½ç»´ç‰¹å¾ï¼šç®€åŒ–ç½‘ç»œ
            config['model']['hidden_layers'] = [64, 32]
            config['model']['dropout_rate'] = max(0.1, config['model']['dropout_rate'] - 0.1)
        
        return config
    
    def _adjust_for_target_complexity(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®ç›®æ ‡å¤æ‚åº¦è°ƒæ•´é…ç½®"""
        if self.n_targets > 5:
            # å¤šç›®æ ‡ï¼šå¢åŠ ç½‘ç»œå®¹é‡
            config['model']['hidden_layers'] = [
                layer * 2 for layer in config['model']['hidden_layers']
            ]
            config['training']['learning_rate'] *= 0.5  # é™ä½å­¦ä¹ ç‡
            
        # åˆ†æç›®æ ‡å€¼çš„åˆ†å¸ƒå¤æ‚åº¦
        target_std = np.std(self.y, axis=0)
        if np.any(target_std > 10):  # é«˜æ–¹å·®ç›®æ ‡
            config['training']['learning_rate'] *= 0.5
            config['training']['early_stopping_patience'] += 5
        
        return config
    
    def analyze_data_characteristics(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®ç‰¹æ€§"""
        characteristics = {
            'sample_size': self.n_samples,
            'feature_dimension': self.n_features,
            'target_dimension': self.n_targets,
            'feature_correlation': self._analyze_feature_correlation(),
            'target_distribution': self._analyze_target_distribution(),
            'data_complexity': self._estimate_data_complexity()
        }
        
        return characteristics
    
    def _analyze_feature_correlation(self) -> Dict[str, float]:
        """åˆ†æç‰¹å¾ç›¸å…³æ€§"""
        corr_matrix = np.corrcoef(self.X.T)
        
        # ç§»é™¤å¯¹è§’çº¿å…ƒç´ 
        mask = ~np.eye(corr_matrix.shape[0], dtype=bool)
        correlations = corr_matrix[mask]
        
        return {
            'max_correlation': np.max(np.abs(correlations)),
            'mean_correlation': np.mean(np.abs(correlations)),
            'high_correlation_pairs': np.sum(np.abs(correlations) > 0.8)
        }
    
    def _analyze_target_distribution(self) -> Dict[str, Any]:
        """åˆ†æç›®æ ‡åˆ†å¸ƒ"""
        return {
            'mean': np.mean(self.y, axis=0).tolist(),
            'std': np.std(self.y, axis=0).tolist(),
            'skewness': self._calculate_skewness(self.y),
            'range': (np.min(self.y, axis=0).tolist(), np.max(self.y, axis=0).tolist())
        }
    
    def _calculate_skewness(self, data: np.ndarray) -> list:
        """è®¡ç®—ååº¦"""
        from scipy import stats
        if data.ndim == 1:
            return [stats.skew(data)]
        else:
            return [stats.skew(data[:, i]) for i in range(data.shape[1])]
    
    def _estimate_data_complexity(self) -> str:
        """ä¼°è®¡æ•°æ®å¤æ‚åº¦"""
        # ç®€å•çš„å¤æ‚åº¦ä¼°è®¡
        feature_ratio = self.n_features / self.n_samples
        
        if feature_ratio > 0.5:
            return "high"  # é«˜ç»´å°æ ·æœ¬
        elif feature_ratio > 0.1:
            return "medium"
        else:
            return "low"

# ä½¿ç”¨ç¤ºä¾‹
def get_adaptive_config(X, y):
    """è·å–è‡ªé€‚åº”é…ç½®"""
    tuner = AdaptiveTuner(X, y)
    
    # åˆ†ææ•°æ®ç‰¹æ€§
    characteristics = tuner.analyze_data_characteristics()
    print("æ•°æ®ç‰¹æ€§åˆ†æ:")
    for key, value in characteristics.items():
        print(f"  {key}: {value}")
    
    # è·å–æ¨èé…ç½®
    recommended_config = tuner.get_recommended_config()
    print("\næ¨èé…ç½®:")
    print(yaml.dump(recommended_config, default_flow_style=False))
    
    return recommended_config
```

### 2. è‡ªåŠ¨è¶…å‚æ•°æœç´¢

```python
# åˆ›å»ºhyperparameter_search.py
import optuna
import numpy as np
from sklearn.model_selection import KFold
from typing import Dict, Any, Callable
import yaml
import joblib

class HyperparameterSearcher:
    """è¶…å‚æ•°æœç´¢å™¨"""
    
    def __init__(self, 
                 X: np.ndarray, 
                 y: np.ndarray,
                 base_config: Dict[str, Any],
                 cv_folds: int = 5,
                 n_trials: int = 100):
        self.X = X
        self.y = y
        self.base_config = base_config
        self.cv_folds = cv_folds
        self.n_trials = n_trials
        self.study = None
        
    def define_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """å®šä¹‰æœç´¢ç©ºé—´"""
        config = self.base_config.copy()
        
        # ç½‘ç»œæ¶æ„æœç´¢
        n_layers = trial.suggest_int('n_layers', 2, 5)
        hidden_layers = []
        
        # ç¬¬ä¸€å±‚å¤§å°
        first_layer = trial.suggest_int('first_layer', 32, 512, log=True)
        hidden_layers.append(first_layer)
        
        # åç»­å±‚é€’å‡
        current_size = first_layer
        for i in range(1, n_layers):
            # ç¡®ä¿å±‚å¤§å°é€’å‡
            max_size = max(16, current_size // 2)
            layer_size = trial.suggest_int(f'layer_{i}', 16, max_size, log=True)
            hidden_layers.append(layer_size)
            current_size = layer_size
        
        config['model']['hidden_layers'] = hidden_layers
        
        # å…¶ä»–è¶…å‚æ•°
        config['model']['activation'] = trial.suggest_categorical(
            'activation', ['relu', 'tanh', 'leaky_relu']
        )
        config['model']['dropout_rate'] = trial.suggest_float('dropout_rate', 0.0, 0.5)
        config['model']['batch_norm'] = trial.suggest_categorical('batch_norm', [True, False])
        
        # è®­ç»ƒå‚æ•°
        config['training']['learning_rate'] = trial.suggest_float(
            'learning_rate', 1e-5, 1e-1, log=True
        )
        config['training']['batch_size'] = trial.suggest_categorical(
            'batch_size', [16, 32, 64, 128]
        )
        config['training']['weight_decay'] = trial.suggest_float(
            'weight_decay', 1e-6, 1e-2, log=True
        )
        
        # ä¼˜åŒ–å™¨
        config['optimizer']['type'] = trial.suggest_categorical(
            'optimizer', ['adam', 'sgd', 'rmsprop']
        )
        
        if config['optimizer']['type'] == 'sgd':
            config['optimizer']['momentum'] = trial.suggest_float('momentum', 0.8, 0.99)
        
        return config
    
    def objective(self, trial: optuna.Trial) -> float:
        """ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        config = self.define_search_space(trial)
        
        # äº¤å‰éªŒè¯
        kf = KFold(n_splits=self.cv_folds, shuffle=True, random_state=42)
        cv_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(self.X)):
            X_train_fold, X_val_fold = self.X[train_idx], self.X[val_idx]
            y_train_fold, y_val_fold = self.y[train_idx], self.y[val_idx]
            
            try:
                # è®­ç»ƒæ¨¡å‹
                score = self._train_and_evaluate(
                    config, X_train_fold, y_train_fold, X_val_fold, y_val_fold
                )
                cv_scores.append(score)
                
            except Exception as e:
                # å¦‚æœè®­ç»ƒå¤±è´¥ï¼Œè¿”å›å¾ˆå·®çš„åˆ†æ•°
                print(f"Trial {trial.number} fold {fold} failed: {e}")
                return -1000.0
        
        # è¿”å›å¹³å‡CVåˆ†æ•°
        mean_score = np.mean(cv_scores)
        
        # æŠ¥å‘Šä¸­é—´ç»“æœ
        trial.report(mean_score, fold)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‰ªæ
        if trial.should_prune():
            raise optuna.TrialPruned()
        
        return mean_score
    
    def _train_and_evaluate(self, 
                           config: Dict[str, Any],
                           X_train: np.ndarray,
                           y_train: np.ndarray,
                           X_val: np.ndarray,
                           y_val: np.ndarray) -> float:
        """è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹"""
        from data_processor import DataProcessor
        from mlp_model import create_model_from_config
        from trainer import MLPTrainer
        
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        processor = DataProcessor(config)
        processor.load_data_from_arrays(
            np.vstack([X_train, X_val]), 
            np.vstack([y_train, y_val])
        )
        processor.normalize_data()
        
        # æ‰‹åŠ¨åˆ†å‰²æ•°æ®ï¼ˆå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†åˆ†å‰²ï¼‰
        train_size = len(X_train)
        X_processed = processor.X_processed
        y_processed = processor.y_processed
        
        X_train_proc = X_processed[:train_size]
        X_val_proc = X_processed[train_size:]
        y_train_proc = y_processed[:train_size]
        y_val_proc = y_processed[train_size:]
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, _ = processor.create_data_loaders(
            X_train_proc, X_val_proc, X_val_proc,  # ä½¿ç”¨valä½œä¸ºtest
            y_train_proc, y_val_proc, y_val_proc
        )
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model_from_config(config, processor.input_dim, processor.output_dim)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MLPTrainer(model, config)
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆè¾ƒå°‘çš„epochsç”¨äºæœç´¢ï¼‰
        config_copy = config.copy()
        config_copy['training']['epochs'] = min(50, config['training']['epochs'])
        config_copy['training']['early_stopping_patience'] = 5
        
        trainer.config = config_copy
        history = trainer.train(train_loader, val_loader)
        
        # è¿”å›æœ€ä½³éªŒè¯RÂ²åˆ†æ•°
        best_r2 = max([metrics['r2'] for metrics in history['val_metrics']])
        return best_r2
    
    def search(self, 
               study_name: str = "mlp_hyperparameter_search",
               storage: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œè¶…å‚æ•°æœç´¢"""
        # åˆ›å»ºæˆ–åŠ è½½study
        if storage:
            self.study = optuna.create_study(
                study_name=study_name,
                storage=storage,
                load_if_exists=True,
                direction='maximize',
                pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
            )
        else:
            self.study = optuna.create_study(
                direction='maximize',
                pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
            )
        
        # æ‰§è¡Œä¼˜åŒ–
        self.study.optimize(self.objective, n_trials=self.n_trials)
        
        # è¿”å›æœ€ä½³å‚æ•°
        best_params = self.study.best_params
        best_config = self._params_to_config(best_params)
        
        return {
            'best_params': best_params,
            'best_config': best_config,
            'best_score': self.study.best_value,
            'study': self.study
        }
    
    def _params_to_config(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """å°†å‚æ•°è½¬æ¢ä¸ºé…ç½®æ ¼å¼"""
        config = self.base_config.copy()
        
        # é‡å»ºhidden_layers
        n_layers = params['n_layers']
        hidden_layers = [params['first_layer']]
        for i in range(1, n_layers):
            hidden_layers.append(params[f'layer_{i}'])
        
        config['model']['hidden_layers'] = hidden_layers
        config['model']['activation'] = params['activation']
        config['model']['dropout_rate'] = params['dropout_rate']
        config['model']['batch_norm'] = params['batch_norm']
        
        config['training']['learning_rate'] = params['learning_rate']
        config['training']['batch_size'] = params['batch_size']
        config['training']['weight_decay'] = params['weight_decay']
        
        config['optimizer']['type'] = params['optimizer']
        if params['optimizer'] == 'sgd':
            config['optimizer']['momentum'] = params.get('momentum', 0.9)
        
        return config
    
    def save_results(self, results: Dict[str, Any], filepath: str):
        """ä¿å­˜æœç´¢ç»“æœ"""
        # ä¿å­˜æœ€ä½³é…ç½®
        with open(f"{filepath}_best_config.yaml", 'w') as f:
            yaml.dump(results['best_config'], f, default_flow_style=False)
        
        # ä¿å­˜studyå¯¹è±¡
        joblib.dump(results['study'], f"{filepath}_study.pkl")
        
        # ä¿å­˜æœç´¢æ‘˜è¦
        summary = {
            'best_score': results['best_score'],
            'best_params': results['best_params'],
            'n_trials': len(results['study'].trials),
            'study_name': results['study'].study_name
        }
        
        with open(f"{filepath}_summary.yaml", 'w') as f:
            yaml.dump(summary, f, default_flow_style=False)
    
    def plot_optimization_history(self, save_path: str = None):
        """ç»˜åˆ¶ä¼˜åŒ–å†å²"""
        if self.study is None:
            raise ValueError("éœ€è¦å…ˆæ‰§è¡Œæœç´¢")
        
        import matplotlib.pyplot as plt
        
        # ä¼˜åŒ–å†å²
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # ç›®æ ‡å€¼å†å²
        trials = self.study.trials
        values = [trial.value for trial in trials if trial.value is not None]
        ax1.plot(values)
        ax1.set_xlabel('Trial')
        ax1.set_ylabel('Objective Value (RÂ²)')
        ax1.set_title('Optimization History')
        ax1.grid(True)
        
        # å‚æ•°é‡è¦æ€§
        try:
            importance = optuna.importance.get_param_importances(self.study)
            params = list(importance.keys())[:10]  # å‰10ä¸ªé‡è¦å‚æ•°
            importances = [importance[param] for param in params]
            
            ax2.barh(params, importances)
            ax2.set_xlabel('Importance')
            ax2.set_title('Parameter Importance')
        except:
            ax2.text(0.5, 0.5, 'Parameter importance\nnot available', 
                    ha='center', va='center', transform=ax2.transAxes)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
def run_hyperparameter_search(X, y, base_config, n_trials=50):
    """è¿è¡Œè¶…å‚æ•°æœç´¢"""
    searcher = HyperparameterSearcher(X, y, base_config, n_trials=n_trials)
    
    print(f"å¼€å§‹è¶…å‚æ•°æœç´¢ï¼Œå…±{n_trials}æ¬¡è¯•éªŒ...")
    results = searcher.search()
    
    print(f"\næœç´¢å®Œæˆï¼")
    print(f"æœ€ä½³åˆ†æ•°: {results['best_score']:.4f}")
    print(f"æœ€ä½³å‚æ•°: {results['best_params']}")
    
    # ä¿å­˜ç»“æœ
    searcher.save_results(results, "hyperparameter_search_results")
    
    # ç»˜åˆ¶ä¼˜åŒ–å†å²
    searcher.plot_optimization_history("optimization_history.png")
    
    return results
```

### 3. å­¦ä¹ ç‡æŸ¥æ‰¾å™¨

```python
# åˆ›å»ºlearning_rate_finder.py
import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List
import math

class LearningRateFinder:
    """å­¦ä¹ ç‡æŸ¥æ‰¾å™¨"""
    
    def __init__(self, model, optimizer, criterion, device):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        
        # ä¿å­˜åˆå§‹çŠ¶æ€
        self.initial_state = model.state_dict().copy()
        self.initial_optimizer_state = optimizer.state_dict().copy()
    
    def find_lr(self, 
                train_loader,
                start_lr: float = 1e-7,
                end_lr: float = 10,
                num_iter: int = None,
                smooth_f: float = 0.05,
                diverge_th: int = 5) -> Tuple[List[float], List[float]]:
        """
        æŸ¥æ‰¾æœ€ä¼˜å­¦ä¹ ç‡
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            start_lr: èµ·å§‹å­¦ä¹ ç‡
            end_lr: ç»“æŸå­¦ä¹ ç‡
            num_iter: è¿­ä»£æ¬¡æ•°
            smooth_f: å¹³æ»‘å› å­
            diverge_th: å‘æ•£é˜ˆå€¼
            
        Returns:
            å­¦ä¹ ç‡åˆ—è¡¨å’ŒæŸå¤±åˆ—è¡¨
        """
        if num_iter is None:
            num_iter = len(train_loader)
        
        # è®¡ç®—å­¦ä¹ ç‡ä¹˜æ•°
        mult = (end_lr / start_lr) ** (1 / num_iter)
        
        # åˆå§‹åŒ–
        lr = start_lr
        self.optimizer.param_groups[0]['lr'] = lr
        
        avg_loss = 0.0
        best_loss = 0.0
        batch_num = 0
        losses = []
        lrs = []
        
        # è®­ç»ƒå¾ªç¯
        for batch_idx, (data, target) in enumerate(train_loader):
            if batch_idx >= num_iter:
                break
                
            batch_num += 1
            
            # å‰å‘ä¼ æ’­
            data, target = data.to(self.device), target.to(self.device)
            self.optimizer.zero_grad()
            outputs = self.model(data)
            loss = self.criterion(outputs, target)
            
            # è®¡ç®—å¹³æ»‘æŸå¤±
            if batch_idx == 0:
                avg_loss = loss.item()
                best_loss = avg_loss
            else:
                avg_loss = smooth_f * loss.item() + (1 - smooth_f) * avg_loss
            
            # æ£€æŸ¥æ˜¯å¦å‘æ•£
            if batch_num > 1 and avg_loss > diverge_th * best_loss:
                print(f"å­¦ä¹ ç‡æŸ¥æ‰¾åœ¨ç¬¬{batch_num}æ¬¡è¿­ä»£æ—¶åœæ­¢ï¼ŒæŸå¤±å‘æ•£")
                break
            
            # æ›´æ–°æœ€ä½³æŸå¤±
            if avg_loss < best_loss or batch_num == 1:
                best_loss = avg_loss
            
            # è®°å½•
            losses.append(avg_loss)
            lrs.append(lr)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # æ›´æ–°å­¦ä¹ ç‡
            lr *= mult
            self.optimizer.param_groups[0]['lr'] = lr
        
        # æ¢å¤åˆå§‹çŠ¶æ€
        self.model.load_state_dict(self.initial_state)
        self.optimizer.load_state_dict(self.initial_optimizer_state)
        
        return lrs, losses
    
    def plot(self, lrs: List[float], losses: List[float], save_path: str = None):
        """ç»˜åˆ¶å­¦ä¹ ç‡vsæŸå¤±æ›²çº¿"""
        plt.figure(figsize=(10, 6))
        plt.plot(lrs, losses)
        plt.xscale('log')
        plt.xlabel('Learning Rate')
        plt.ylabel('Loss')
        plt.title('Learning Rate Finder')
        plt.grid(True)
        
        # æ ‡è®°å»ºè®®çš„å­¦ä¹ ç‡
        min_loss_idx = np.argmin(losses)
        suggested_lr = lrs[min_loss_idx] / 10  # é€šå¸¸é€‰æ‹©æœ€å°æŸå¤±å¯¹åº”å­¦ä¹ ç‡çš„1/10
        
        plt.axvline(x=suggested_lr, color='red', linestyle='--', 
                   label=f'Suggested LR: {suggested_lr:.2e}')
        plt.legend()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        return suggested_lr
    
    def suggest_lr(self, lrs: List[float], losses: List[float]) -> float:
        """å»ºè®®æœ€ä¼˜å­¦ä¹ ç‡"""
        # æ–¹æ³•1: æœ€å°æŸå¤±å¯¹åº”å­¦ä¹ ç‡çš„1/10
        min_loss_idx = np.argmin(losses)
        lr_at_min_loss = lrs[min_loss_idx]
        
        # æ–¹æ³•2: æœ€å¤§æ¢¯åº¦ä¸‹é™é€Ÿç‡å¯¹åº”çš„å­¦ä¹ ç‡
        gradients = np.gradient(losses)
        max_gradient_idx = np.argmin(gradients)  # æœ€è´Ÿçš„æ¢¯åº¦
        lr_at_max_gradient = lrs[max_gradient_idx]
        
        # ç»¼åˆå»ºè®®
        suggested_lr = min(lr_at_min_loss / 10, lr_at_max_gradient)
        
        print(f"æœ€å°æŸå¤±å¯¹åº”å­¦ä¹ ç‡: {lr_at_min_loss:.2e}")
        print(f"æœ€å¤§æ¢¯åº¦ä¸‹é™å¯¹åº”å­¦ä¹ ç‡: {lr_at_max_gradient:.2e}")
        print(f"å»ºè®®å­¦ä¹ ç‡: {suggested_lr:.2e}")
        
        return suggested_lr

# ä½¿ç”¨ç¤ºä¾‹
def find_optimal_learning_rate(model, train_loader, config):
    """æŸ¥æ‰¾æœ€ä¼˜å­¦ä¹ ç‡"""
    from trainer import MLPTrainer
    
    # åˆ›å»ºä¸´æ—¶è®­ç»ƒå™¨
    trainer = MLPTrainer(model, config)
    
    # åˆ›å»ºå­¦ä¹ ç‡æŸ¥æ‰¾å™¨
    lr_finder = LearningRateFinder(
        model=trainer.model,
        optimizer=trainer.optimizer,
        criterion=trainer.criterion,
        device=trainer.device
    )
    
    # æŸ¥æ‰¾å­¦ä¹ ç‡
    lrs, losses = lr_finder.find_lr(train_loader)
    
    # ç»˜åˆ¶ç»“æœå¹¶è·å–å»ºè®®
    suggested_lr = lr_finder.plot(lrs, losses, "learning_rate_finder.png")
    
    return suggested_lr
```

### 4. æ‰¹é‡å¤§å°ä¼˜åŒ–

```python
# åˆ›å»ºbatch_size_optimizer.py
import torch
import numpy as np
import time
from typing import Dict, List, Tuple

class BatchSizeOptimizer:
    """æ‰¹é‡å¤§å°ä¼˜åŒ–å™¨"""
    
    def __init__(self, model, train_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def find_optimal_batch_size(self, 
                               batch_sizes: List[int] = None,
                               max_memory_usage: float = 0.8) -> Dict[str, any]:
        """
        æŸ¥æ‰¾æœ€ä¼˜æ‰¹é‡å¤§å°
        
        Args:
            batch_sizes: è¦æµ‹è¯•çš„æ‰¹é‡å¤§å°åˆ—è¡¨
            max_memory_usage: æœ€å¤§å†…å­˜ä½¿ç”¨ç‡
            
        Returns:
            ä¼˜åŒ–ç»“æœå­—å…¸
        """
        if batch_sizes is None:
            batch_sizes = [8, 16, 32, 64, 128, 256, 512]
        
        results = []
        
        for batch_size in batch_sizes:
            print(f"æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")
            
            try:
                result = self._test_batch_size(batch_size, max_memory_usage)
                if result:
                    results.append(result)
                    print(f"  æˆåŠŸ: æ—¶é—´={result['time_per_batch']:.3f}s, "
                          f"å†…å­˜={result['memory_usage']:.1f}MB")
                else:
                    print(f"  å¤±è´¥: å†…å­˜ä¸è¶³æˆ–å…¶ä»–é”™è¯¯")
                    
            except Exception as e:
                print(f"  é”™è¯¯: {e}")
                continue
        
        if not results:
            raise RuntimeError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ‰¹é‡å¤§å°")
        
        # åˆ†æç»“æœ
        optimal_result = self._analyze_results(results)
        
        return optimal_result
    
    def _test_batch_size(self, batch_size: int, max_memory_usage: float) -> Dict:
        """æµ‹è¯•ç‰¹å®šæ‰¹é‡å¤§å°"""
        from data_processor import DataProcessor
        from trainer import MLPTrainer
        
        # åˆ›å»ºæ–°çš„æ•°æ®åŠ è½½å™¨
        dataset = self.train_loader.dataset
        test_loader = torch.utils.data.DataLoader(
            dataset, batch_size=batch_size, shuffle=False, num_workers=0
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        config_copy = self.config.copy()
        config_copy['training']['batch_size'] = batch_size
        trainer = MLPTrainer(self.model, config_copy)
        
        # æ¸…ç©ºGPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
        
        # æµ‹è¯•å‡ ä¸ªæ‰¹æ¬¡
        times = []
        max_memory = 0
        
        trainer.model.train()
        
        for i, (data, target) in enumerate(test_loader):
            if i >= 5:  # åªæµ‹è¯•5ä¸ªæ‰¹æ¬¡
                break
                
            start_time = time.time()
            
            try:
                data, target = data.to(trainer.device), target.to(trainer.device)
                
                trainer.optimizer.zero_grad()
                output = trainer.model(data)
                loss = trainer.criterion(output, target)
                loss.backward()
                trainer.optimizer.step()
                
                end_time = time.time()
                times.append(end_time - start_time)
                
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨
                if torch.cuda.is_available():
                    current_memory = torch.cuda.memory_allocated()
                    max_memory = max(max_memory, current_memory - initial_memory)
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å†…å­˜é™åˆ¶
                    total_memory = torch.cuda.get_device_properties(0).total_memory
                    if current_memory / total_memory > max_memory_usage:
                        return None
                        
            except RuntimeError as e:
                if "out of memory" in str(e):
                    return None
                else:
                    raise e
        
        if not times:
            return None
        
        return {
            'batch_size': batch_size,
            'time_per_batch': np.mean(times),
            'time_std': np.std(times),
            'memory_usage': max_memory / (1024 * 1024),  # MB
            'throughput': batch_size / np.mean(times)  # samples/second
        }
    
    def _analyze_results(self, results: List[Dict]) -> Dict:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        # æŒ‰ååé‡æ’åº
        results_sorted = sorted(results, key=lambda x: x['throughput'], reverse=True)
        
        # æ‰¾åˆ°æœ€ä½³æ‰¹é‡å¤§å°ï¼ˆå¹³è¡¡ååé‡å’Œå†…å­˜ä½¿ç”¨ï¼‰
        best_result = results_sorted[0]
        
        # è®¡ç®—æ•ˆç‡åˆ†æ•°ï¼ˆååé‡/å†…å­˜ä½¿ç”¨ï¼‰
        for result in results:
            result['efficiency_score'] = result['throughput'] / (result['memory_usage'] + 1)
        
        # æŒ‰æ•ˆç‡åˆ†æ•°æ’åº
        efficiency_sorted = sorted(results, key=lambda x: x['efficiency_score'], reverse=True)
        most_efficient = efficiency_sorted[0]
        
        return {
            'all_results': results,
            'best_throughput': best_result,
            'most_efficient': most_efficient,
            'recommended': most_efficient,  # æ¨èæœ€é«˜æ•ˆçš„
            'summary': {
                'tested_batch_sizes': [r['batch_size'] for r in results],
                'best_throughput_batch_size': best_result['batch_size'],
                'most_efficient_batch_size': most_efficient['batch_size']
            }
        }
    
    def plot_results(self, results: Dict, save_path: str = None):
        """ç»˜åˆ¶æ‰¹é‡å¤§å°ä¼˜åŒ–ç»“æœ"""
        import matplotlib.pyplot as plt
        
        all_results = results['all_results']
        batch_sizes = [r['batch_size'] for r in all_results]
        throughputs = [r['throughput'] for r in all_results]
        memory_usages = [r['memory_usage'] for r in all_results]
        efficiency_scores = [r['efficiency_score'] for r in all_results]
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # ååé‡
        ax1.plot(batch_sizes, throughputs, 'bo-')
        ax1.set_xlabel('Batch Size')
        ax1.set_ylabel('Throughput (samples/sec)')
        ax1.set_title('Throughput vs Batch Size')
        ax1.grid(True)
        
        # å†…å­˜ä½¿ç”¨
        ax2.plot(batch_sizes, memory_usages, 'ro-')
        ax2.set_xlabel('Batch Size')
        ax2.set_ylabel('Memory Usage (MB)')
        ax2.set_title('Memory Usage vs Batch Size')
        ax2.grid(True)
        
        # æ•ˆç‡åˆ†æ•°
        ax3.plot(batch_sizes, efficiency_scores, 'go-')
        ax3.set_xlabel('Batch Size')
        ax3.set_ylabel('Efficiency Score')
        ax3.set_title('Efficiency Score vs Batch Size')
        ax3.grid(True)
        
        # æ¨èçš„æ‰¹é‡å¤§å°
        recommended_bs = results['recommended']['batch_size']
        ax3.axvline(x=recommended_bs, color='red', linestyle='--', 
                   label=f'Recommended: {recommended_bs}')
        ax3.legend()
        
        # æ—¶é—´å¯¹æ¯”
        times = [r['time_per_batch'] for r in all_results]
        ax4.plot(batch_sizes, times, 'mo-')
        ax4.set_xlabel('Batch Size')
        ax4.set_ylabel('Time per Batch (sec)')
        ax4.set_title('Time per Batch vs Batch Size')
        ax4.grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
def optimize_batch_size(model, train_loader, config):
    """ä¼˜åŒ–æ‰¹é‡å¤§å°"""
    optimizer = BatchSizeOptimizer(model, train_loader, config)
    
    print("å¼€å§‹æ‰¹é‡å¤§å°ä¼˜åŒ–...")
    results = optimizer.find_optimal_batch_size()
    
    print(f"\nä¼˜åŒ–å®Œæˆï¼")
    print(f"æ¨èæ‰¹é‡å¤§å°: {results['recommended']['batch_size']}")
    print(f"ååé‡: {results['recommended']['throughput']:.1f} samples/sec")
    print(f"å†…å­˜ä½¿ç”¨: {results['recommended']['memory_usage']:.1f} MB")
    
    # ç»˜åˆ¶ç»“æœ
    optimizer.plot_results(results, "batch_size_optimization.png")
    
    return results['recommended']['batch_size']
```

## ğŸ“Š è¶…å‚æ•°è°ƒä¼˜å®æˆ˜æŒ‡å—

### 1. è°ƒä¼˜æµç¨‹

```python
# åˆ›å»ºtuning_pipeline.py
def complete_hyperparameter_tuning(X, y, base_config, output_dir="tuning_results"):
    """å®Œæ•´çš„è¶…å‚æ•°è°ƒä¼˜æµç¨‹"""
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    print("=== å¼€å§‹å®Œæ•´è¶…å‚æ•°è°ƒä¼˜æµç¨‹ ===\n")
    
    # æ­¥éª¤1: æ•°æ®ç‰¹æ€§åˆ†æå’Œè‡ªé€‚åº”é…ç½®
    print("1. æ•°æ®ç‰¹æ€§åˆ†æ...")
    tuner = AdaptiveTuner(X, y)
    characteristics = tuner.analyze_data_characteristics()
    adaptive_config = tuner.get_recommended_config()
    
    # ä¿å­˜åˆ†æç»“æœ
    with open(f"{output_dir}/data_characteristics.yaml", 'w') as f:
        yaml.dump(characteristics, f, default_flow_style=False)
    
    with open(f"{output_dir}/adaptive_config.yaml", 'w') as f:
        yaml.dump(adaptive_config, f, default_flow_style=False)
    
    print(f"   æ•°æ®ç‰¹æ€§å·²ä¿å­˜åˆ° {output_dir}/data_characteristics.yaml")
    print(f"   è‡ªé€‚åº”é…ç½®å·²ä¿å­˜åˆ° {output_dir}/adaptive_config.yaml\n")
    
    # æ­¥éª¤2: å­¦ä¹ ç‡æŸ¥æ‰¾
    print("2. å­¦ä¹ ç‡ä¼˜åŒ–...")
    from data_processor import DataProcessor
    from mlp_model import create_model_from_config
    
    processor = DataProcessor(adaptive_config)
    processor.load_data_from_arrays(X, y)
    processor.normalize_data()
    
    X_train, X_val, X_test, y_train, y_val, y_test = processor.split_data()
    train_loader, val_loader, test_loader = processor.create_data_loaders(
        X_train, X_val, X_test, y_train, y_val, y_test
    )
    
    model = create_model_from_config(adaptive_config, processor.input_dim, processor.output_dim)
    optimal_lr = find_optimal_learning_rate(model, train_loader, adaptive_config)
    
    # æ›´æ–°é…ç½®
    adaptive_config['training']['learning_rate'] = optimal_lr
    print(f"   æœ€ä¼˜å­¦ä¹ ç‡: {optimal_lr:.2e}\n")
    
    # æ­¥éª¤3: æ‰¹é‡å¤§å°ä¼˜åŒ–
    print("3. æ‰¹é‡å¤§å°ä¼˜åŒ–...")
    optimal_batch_size = optimize_batch_size(model, train_loader, adaptive_config)
    adaptive_config['training']['batch_size'] = optimal_batch_size
    print(f"   æœ€ä¼˜æ‰¹é‡å¤§å°: {optimal_batch_size}\n")
    
    # æ­¥éª¤4: å…¨é¢è¶…å‚æ•°æœç´¢
    print("4. å…¨é¢è¶…å‚æ•°æœç´¢...")
    searcher = HyperparameterSearcher(X, y, adaptive_config, n_trials=100)
    search_results = searcher.search()
    
    # ä¿å­˜æœç´¢ç»“æœ
    searcher.save_results(search_results, f"{output_dir}/hyperparameter_search")
    searcher.plot_optimization_history(f"{output_dir}/optimization_history.png")
    
    print(f"   æœ€ä½³è¶…å‚æ•°æœç´¢åˆ†æ•°: {search_results['best_score']:.4f}")
    print(f"   æœç´¢ç»“æœå·²ä¿å­˜åˆ° {output_dir}/\n")
    
    # æ­¥éª¤5: æœ€ç»ˆéªŒè¯
    print("5. æœ€ç»ˆæ¨¡å‹éªŒè¯...")
    final_config = search_results['best_config']
    
    # ä½¿ç”¨æœ€ä½³é…ç½®è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_model = create_model_from_config(final_config, processor.input_dim, processor.output_dim)
    from trainer import MLPTrainer
    final_trainer = MLPTrainer(final_model, final_config)
    
    # å®Œæ•´è®­ç»ƒ
    final_config['training']['epochs'] = 100  # æ¢å¤å®Œæ•´è®­ç»ƒè½®æ•°
    final_trainer.config = final_config
    history = final_trainer.train(train_loader, val_loader)
    
    # è¯„ä¼°
    from evaluator import ModelEvaluator
    evaluator = ModelEvaluator(save_plots=True, plot_dir=output_dir)
    
    test_pred, test_true = final_trainer.predict(test_loader)
    test_pred_orig = processor.inverse_transform_predictions(test_pred)
    test_true_orig = processor.inverse_transform_predictions(test_true)
    
    final_metrics = evaluator.evaluate_model(test_true_orig, test_pred_orig, "final_test")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results = {
        'final_config': final_config,
        'final_metrics': final_metrics,
        'model_info': final_model.get_model_info(),
        'training_summary': final_trainer.get_training_summary()
    }
    
    with open(f"{output_dir}/final_results.yaml", 'w') as f:
        yaml.dump(final_results, f, default_flow_style=False)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_trainer.save_model(f"{output_dir}/final_model.pth")
    processor.save_scalers(output_dir)
    
    print(f"   æœ€ç»ˆæµ‹è¯•RÂ²åˆ†æ•°: {final_metrics['r2']:.4f}")
    print(f"   æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}/final_model.pth")
    
    print("\n=== è¶…å‚æ•°è°ƒä¼˜å®Œæˆ ===")
    
    return final_results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from data_processor import generate_sample_data
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    X, y = generate_sample_data(n_samples=2000, n_features=15, n_targets=3)
    
    # åŸºç¡€é…ç½®
    base_config = {
        'data': {'train_ratio': 0.8, 'val_ratio': 0.1, 'test_ratio': 0.1, 
                'random_seed': 42, 'normalize': True},
        'model': {'hidden_layers': [128, 64, 32], 'activation': 'relu', 
                 'dropout_rate': 0.2, 'batch_norm': False},
        'training': {'batch_size': 32, 'epochs': 100, 'learning_rate': 0.001, 
                    'weight_decay': 1e-5, 'early_stopping_patience': 10},
        'optimizer': {'type': 'adam'}
    }
    
    # æ‰§è¡Œå®Œæ•´è°ƒä¼˜
    results = complete_hyperparameter_tuning(X, y, base_config)
```

è¿™ä¸ªè¶…å‚æ•°è°ƒä¼˜æŒ‡å—æä¾›äº†ç³»ç»Ÿæ€§çš„è°ƒä¼˜æ–¹æ³•å’Œå·¥å…·ï¼Œå¸®åŠ©æ‚¨æ ¹æ®æ•°æ®ç‰¹æ€§æ‰¾åˆ°æœ€ä¼˜çš„æ¨¡å‹é…ç½®ã€‚é€šè¿‡è‡ªé€‚åº”é…ç½®ã€å­¦ä¹ ç‡æŸ¥æ‰¾ã€æ‰¹é‡å¤§å°ä¼˜åŒ–å’Œå…¨é¢æœç´¢çš„ç»„åˆï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚