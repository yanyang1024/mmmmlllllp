# MLPæ•°å€¼é¢„æµ‹æ¨¡å‹ä¼˜åŒ–æŒ‡å—

æœ¬æ–‡æ¡£æä¾›äº†é’ˆå¯¹ä¸åŒæ•°æ®ç‰¹æ€§å’Œéœ€æ±‚çš„ç½‘ç»œç»“æ„è°ƒæ•´ã€è¶…å‚æ•°ä¼˜åŒ–ä»¥åŠäºŒæ¬¡å¼€å‘çš„è¯¦ç»†æŒ‡å¯¼ã€‚

## ğŸ“Š æ•°æ®ç‰¹æ€§åˆ†æä¸ç½‘ç»œç»“æ„è°ƒæ•´

### 1. æ ¹æ®æ•°æ®è§„æ¨¡è°ƒæ•´ç½‘ç»œç»“æ„

#### å°æ•°æ®é›† (< 1000 æ ·æœ¬)
```yaml
# config.yaml å»ºè®®é…ç½®
model:
  hidden_layers: [32, 16]  # è¾ƒå°çš„ç½‘ç»œé¿å…è¿‡æ‹Ÿåˆ
  dropout_rate: 0.1        # è¾ƒä½çš„dropout
  activation: "relu"

training:
  batch_size: 16           # è¾ƒå°çš„batch size
  epochs: 200              # æ›´å¤šçš„epochs
  learning_rate: 0.01      # è¾ƒé«˜çš„å­¦ä¹ ç‡
  early_stopping_patience: 20
```

**è°ƒæ•´åŸå› **ï¼š
- å°ç½‘ç»œå‡å°‘å‚æ•°æ•°é‡ï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©
- å°batch sizeå¢åŠ æ¢¯åº¦å™ªå£°ï¼Œæœ‰åŠ©äºæ³›åŒ–
- æ›´å¤šepochsç¡®ä¿å……åˆ†è®­ç»ƒ

#### ä¸­ç­‰æ•°æ®é›† (1000-10000 æ ·æœ¬)
```yaml
# é»˜è®¤é…ç½®é€‚ç”¨
model:
  hidden_layers: [128, 64, 32]
  dropout_rate: 0.2
  activation: "relu"

training:
  batch_size: 32
  epochs: 100
  learning_rate: 0.001
```

#### å¤§æ•°æ®é›† (> 10000 æ ·æœ¬)
```yaml
model:
  hidden_layers: [256, 128, 64, 32]  # æ›´æ·±çš„ç½‘ç»œ
  dropout_rate: 0.3                  # æ›´é«˜çš„dropout
  activation: "relu"
  batch_norm: true                   # å¯ç”¨æ‰¹æ ‡å‡†åŒ–

training:
  batch_size: 64                     # æ›´å¤§çš„batch size
  epochs: 50                         # è¾ƒå°‘çš„epochs
  learning_rate: 0.0001              # è¾ƒä½çš„å­¦ä¹ ç‡
```

### 2. æ ¹æ®ç‰¹å¾ç»´åº¦è°ƒæ•´

#### é«˜ç»´ç‰¹å¾ (> 100 ç»´)
```python
# åœ¨data_processor.pyä¸­æ·»åŠ ç‰¹å¾é€‰æ‹©
from sklearn.feature_selection import SelectKBest, f_regression

class DataProcessor:
    def feature_selection(self, k=50):
        """ç‰¹å¾é€‰æ‹©ï¼Œä¿ç•™æœ€é‡è¦çš„kä¸ªç‰¹å¾"""
        selector = SelectKBest(score_func=f_regression, k=k)
        self.X_processed = selector.fit_transform(self.X_processed, self.y_processed)
        self.feature_selector = selector
        logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œä¿ç•™{k}ä¸ªç‰¹å¾")
```

**ç½‘ç»œç»“æ„å»ºè®®**ï¼š
```yaml
model:
  hidden_layers: [512, 256, 128, 64]  # ç¬¬ä¸€å±‚è¾ƒå¤§ï¼Œé€æ¸å‡å°
  dropout_rate: 0.4                   # é«˜dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
```

#### ä½ç»´ç‰¹å¾ (< 10 ç»´)
```yaml
model:
  hidden_layers: [64, 32]             # ç®€å•ç½‘ç»œç»“æ„
  dropout_rate: 0.1                   # ä½dropout
```

### 3. æ ¹æ®ç›®æ ‡å˜é‡ç‰¹æ€§è°ƒæ•´

#### å¤šç›®æ ‡å›å½’ (m > 5)
```python
# åœ¨mlp_model.pyä¸­æ·»åŠ å¤šä»»åŠ¡å­¦ä¹ æ”¯æŒ
class MultiTaskMLPModel(MLPModel):
    def __init__(self, input_dim, output_dims_list, shared_layers=[128, 64], **kwargs):
        """
        å¤šä»»åŠ¡MLPæ¨¡å‹
        
        Args:
            output_dims_list: æ¯ä¸ªä»»åŠ¡çš„è¾“å‡ºç»´åº¦åˆ—è¡¨
            shared_layers: å…±äº«å±‚ç»“æ„
        """
        super().__init__(input_dim, sum(output_dims_list), shared_layers, **kwargs)
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸“é—¨çš„è¾“å‡ºå±‚
        self.task_heads = nn.ModuleList()
        for output_dim in output_dims_list:
            self.task_heads.append(nn.Linear(shared_layers[-1], output_dim))
```

#### ç›®æ ‡å€¼èŒƒå›´å·®å¼‚å¤§
```python
# åœ¨data_processor.pyä¸­æ·»åŠ ç›®æ ‡å€¼åˆ†åˆ«æ ‡å‡†åŒ–
def normalize_targets_separately(self):
    """å¯¹æ¯ä¸ªç›®æ ‡å˜é‡åˆ†åˆ«è¿›è¡Œæ ‡å‡†åŒ–"""
    self.target_scalers = []
    normalized_targets = []
    
    for i in range(self.y_raw.shape[1]):
        scaler = StandardScaler()
        target_normalized = scaler.fit_transform(self.y_raw[:, i:i+1])
        normalized_targets.append(target_normalized)
        self.target_scalers.append(scaler)
    
    self.y_processed = np.hstack(normalized_targets)
```

## ğŸ¯ è¶…å‚æ•°ä¼˜åŒ–ç­–ç•¥

### 1. ç³»ç»Ÿæ€§è¶…å‚æ•°æœç´¢

```python
# åˆ›å»ºhyperparameter_tuning.py
import optuna
from sklearn.model_selection import cross_val_score

class HyperparameterTuner:
    def __init__(self, base_config, X, y):
        self.base_config = base_config
        self.X = X
        self.y = y
    
    def objective(self, trial):
        """Optunaä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        # å»ºè®®æœç´¢ç©ºé—´
        config = self.base_config.copy()
        
        # ç½‘ç»œç»“æ„å‚æ•°
        n_layers = trial.suggest_int('n_layers', 2, 5)
        hidden_sizes = []
        for i in range(n_layers):
            size = trial.suggest_int(f'hidden_size_{i}', 16, 512, log=True)
            hidden_sizes.append(size)
        
        config['model']['hidden_layers'] = hidden_sizes
        config['model']['dropout_rate'] = trial.suggest_float('dropout_rate', 0.0, 0.5)
        config['model']['activation'] = trial.suggest_categorical('activation', 
                                                                 ['relu', 'tanh', 'leaky_relu'])
        
        # è®­ç»ƒå‚æ•°
        config['training']['learning_rate'] = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)
        config['training']['batch_size'] = trial.suggest_categorical('batch_size', [16, 32, 64, 128])
        config['training']['weight_decay'] = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)
        
        # äº¤å‰éªŒè¯è¯„ä¼°
        score = self.evaluate_config(config)
        return score
    
    def tune(self, n_trials=100):
        """æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        study = optuna.create_study(direction='maximize')
        study.optimize(self.objective, n_trials=n_trials)
        
        return study.best_params, study.best_value
```

### 2. å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

```python
# åœ¨trainer.pyä¸­æ·»åŠ æ›´å¤šå­¦ä¹ ç‡è°ƒåº¦é€‰é¡¹
def _create_advanced_scheduler(self):
    """åˆ›å»ºé«˜çº§å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    scheduler_type = self.config.get('scheduler', {}).get('type', 'plateau')
    
    if scheduler_type == 'cosine':
        return optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=self.config['training']['epochs']
        )
    elif scheduler_type == 'step':
        return optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=30,
            gamma=0.1
        )
    elif scheduler_type == 'exponential':
        return optim.lr_scheduler.ExponentialLR(
            self.optimizer,
            gamma=0.95
        )
    else:
        return self._create_scheduler()  # é»˜è®¤plateau
```

### 3. æ—©åœç­–ç•¥ä¼˜åŒ–

```python
# åœ¨mlp_model.pyä¸­æ·»åŠ é«˜çº§æ—©åœ
class AdvancedEarlyStopping(EarlyStopping):
    def __init__(self, patience=10, min_delta=1e-6, warmup_epochs=10, **kwargs):
        super().__init__(patience, min_delta, **kwargs)
        self.warmup_epochs = warmup_epochs
        self.epoch_count = 0
    
    def __call__(self, val_loss, model):
        self.epoch_count += 1
        
        # é¢„çƒ­æœŸä¸è¿›è¡Œæ—©åœ
        if self.epoch_count <= self.warmup_epochs:
            return False
            
        return super().__call__(val_loss, model)
```

## ğŸ”§ äºŒæ¬¡å¼€å‘æŒ‡å—

### 1. æ¨¡å‹æ¶æ„æ‰©å±•

#### æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
```python
# åœ¨mlp_model.pyä¸­æ·»åŠ 
class AttentionMLP(MLPModel):
    def __init__(self, *args, use_attention=True, **kwargs):
        super().__init__(*args, **kwargs)
        if use_attention:
            self.attention = nn.MultiheadAttention(
                embed_dim=self.hidden_layers[0],
                num_heads=8,
                batch_first=True
            )
    
    def forward(self, x):
        # æ ‡å‡†MLPå‰å‘ä¼ æ’­
        x = super().forward_to_last_hidden(x)  # éœ€è¦ä¿®æ”¹åŸºç±»
        
        # æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
        if hasattr(self, 'attention'):
            x = x.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
            x, _ = self.attention(x, x, x)
            x = x.squeeze(1)
        
        return self.output_layer(x)
```

#### æ®‹å·®è¿æ¥
```python
class ResidualMLP(MLPModel):
    def forward(self, x):
        residual = x
        
        for i, layer in enumerate(self.layers):
            x = layer(x)
            
            # æ‰¹æ ‡å‡†åŒ–
            if self.batch_norm and self.batch_norms is not None:
                x = self.batch_norms[i](x)
            
            # æ¿€æ´»å‡½æ•°
            x = self._get_activation(x)
            
            # æ®‹å·®è¿æ¥ï¼ˆç»´åº¦åŒ¹é…æ—¶ï¼‰
            if i > 0 and x.shape == residual.shape:
                x = x + residual
            residual = x
            
            # Dropout
            x = self.dropouts[i](x)
        
        return self.output_layer(x)
```

### 2. æŸå¤±å‡½æ•°æ‰©å±•

```python
# åˆ›å»ºcustom_losses.py
import torch
import torch.nn as nn

class HuberLoss(nn.Module):
    def __init__(self, delta=1.0):
        super().__init__()
        self.delta = delta
    
    def forward(self, pred, target):
        error = torch.abs(pred - target)
        quadratic = torch.clamp(error, max=self.delta)
        linear = error - quadratic
        return torch.mean(0.5 * quadratic**2 + self.delta * linear)

class QuantileLoss(nn.Module):
    def __init__(self, quantiles=[0.1, 0.5, 0.9]):
        super().__init__()
        self.quantiles = quantiles
    
    def forward(self, pred, target):
        # pred shape: (batch_size, len(quantiles))
        losses = []
        for i, q in enumerate(self.quantiles):
            error = target - pred[:, i:i+1]
            loss = torch.max(q * error, (q - 1) * error)
            losses.append(loss)
        return torch.mean(torch.cat(losses, dim=1))
```

### 3. æ•°æ®å¢å¼ºç­–ç•¥

```python
# åœ¨data_processor.pyä¸­æ·»åŠ 
class DataAugmenter:
    def __init__(self, noise_std=0.01, dropout_rate=0.1):
        self.noise_std = noise_std
        self.dropout_rate = dropout_rate
    
    def add_noise(self, X):
        """æ·»åŠ é«˜æ–¯å™ªå£°"""
        noise = np.random.normal(0, self.noise_std, X.shape)
        return X + noise
    
    def feature_dropout(self, X):
        """éšæœºä¸¢å¼ƒç‰¹å¾"""
        mask = np.random.random(X.shape) > self.dropout_rate
        return X * mask
    
    def mixup(self, X, y, alpha=0.2):
        """Mixupæ•°æ®å¢å¼º"""
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
        
        batch_size = X.shape[0]
        index = np.random.permutation(batch_size)
        
        mixed_X = lam * X + (1 - lam) * X[index]
        mixed_y = lam * y + (1 - lam) * y[index]
        
        return mixed_X, mixed_y
```

### 4. æ¨¡å‹è§£é‡Šæ€§å·¥å…·

```python
# åˆ›å»ºmodel_interpretation.py
import shap
import numpy as np
from sklearn.inspection import permutation_importance

class ModelInterpreter:
    def __init__(self, model, X_background):
        self.model = model
        self.X_background = X_background
        
    def shap_analysis(self, X_test):
        """SHAPå€¼åˆ†æ"""
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.DeepExplainer(self.model, self.X_background)
        shap_values = explainer.shap_values(X_test)
        
        return shap_values
    
    def feature_importance(self, X_test, y_test):
        """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        def model_predict(X):
            with torch.no_grad():
                return self.model(torch.FloatTensor(X)).numpy()
        
        result = permutation_importance(
            model_predict, X_test, y_test,
            n_repeats=10, random_state=42
        )
        
        return result.importances_mean, result.importances_std
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. è®¡ç®—ä¼˜åŒ–

```python
# åœ¨trainer.pyä¸­æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

class OptimizedMLPTrainer(MLPTrainer):
    def __init__(self, *args, use_amp=True, **kwargs):
        super().__init__(*args, **kwargs)
        self.use_amp = use_amp and torch.cuda.is_available()
        if self.use_amp:
            self.scaler = GradScaler()
    
    def train_epoch_optimized(self, train_loader):
        """ä¼˜åŒ–çš„è®­ç»ƒepoch"""
        self.model.train()
        total_loss = 0.0
        
        for data, target in train_loader:
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            
            if self.use_amp:
                with autocast():
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                self.optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(train_loader)
```

### 2. å†…å­˜ä¼˜åŒ–

```python
# æ¢¯åº¦ç´¯ç§¯å‡å°‘å†…å­˜ä½¿ç”¨
def train_with_gradient_accumulation(self, train_loader, accumulation_steps=4):
    """æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ"""
    self.model.train()
    total_loss = 0.0
    
    for i, (data, target) in enumerate(train_loader):
        data, target = data.to(self.device), target.to(self.device)
        
        output = self.model(data)
        loss = self.criterion(output, target) / accumulation_steps
        loss.backward()
        
        if (i + 1) % accumulation_steps == 0:
            self.optimizer.step()
            self.optimizer.zero_grad()
        
        total_loss += loss.item() * accumulation_steps
    
    return total_loss / len(train_loader)
```

## ğŸ¨ å¯è§†åŒ–å¢å¼º

```python
# åˆ›å»ºadvanced_visualization.py
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
import plotly.graph_objects as go
import plotly.express as px

class AdvancedVisualizer:
    def __init__(self, save_dir="advanced_plots"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
    
    def plot_feature_correlation_heatmap(self, X, feature_names=None):
        """ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾"""
        corr_matrix = np.corrcoef(X.T)
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(corr_matrix, 
                   xticklabels=feature_names, 
                   yticklabels=feature_names,
                   annot=True, cmap='coolwarm', center=0)
        plt.title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
        plt.tight_layout()
        plt.savefig(f"{self.save_dir}/feature_correlation.png", dpi=300)
        plt.show()
    
    def plot_learning_curves_interactive(self, history):
        """äº¤äº’å¼å­¦ä¹ æ›²çº¿"""
        epochs = list(range(1, len(history['train_loss']) + 1))
        
        fig = go.Figure()
        
        # è®­ç»ƒæŸå¤±
        fig.add_trace(go.Scatter(
            x=epochs, y=history['train_loss'],
            mode='lines', name='è®­ç»ƒæŸå¤±',
            line=dict(color='blue')
        ))
        
        # éªŒè¯æŸå¤±
        fig.add_trace(go.Scatter(
            x=epochs, y=history['val_loss'],
            mode='lines', name='éªŒè¯æŸå¤±',
            line=dict(color='red')
        ))
        
        fig.update_layout(
            title='æ¨¡å‹è®­ç»ƒå†å²',
            xaxis_title='Epoch',
            yaxis_title='æŸå¤±',
            hovermode='x unified'
        )
        
        fig.write_html(f"{self.save_dir}/learning_curves.html")
        fig.show()
    
    def plot_prediction_distribution(self, y_true, y_pred):
        """é¢„æµ‹åˆ†å¸ƒå¯¹æ¯”"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # çœŸå®å€¼åˆ†å¸ƒ
        axes[0].hist(y_true.flatten(), bins=50, alpha=0.7, label='çœŸå®å€¼')
        axes[0].hist(y_pred.flatten(), bins=50, alpha=0.7, label='é¢„æµ‹å€¼')
        axes[0].set_xlabel('å€¼')
        axes[0].set_ylabel('é¢‘æ¬¡')
        axes[0].set_title('å€¼åˆ†å¸ƒå¯¹æ¯”')
        axes[0].legend()
        
        # Q-Qå›¾
        from scipy import stats
        stats.probplot(y_true.flatten() - y_pred.flatten(), dist="norm", plot=axes[1])
        axes[1].set_title('æ®‹å·®Q-Qå›¾')
        
        plt.tight_layout()
        plt.savefig(f"{self.save_dir}/prediction_distribution.png", dpi=300)
        plt.show()
```

## ğŸ”„ æŒç»­é›†æˆå’Œéƒ¨ç½²

```python
# åˆ›å»ºmodel_deployment.py
import torch
import onnx
import onnxruntime as ort

class ModelDeployment:
    def __init__(self, model, example_input):
        self.model = model
        self.example_input = example_input
    
    def export_to_onnx(self, filepath="model.onnx"):
        """å¯¼å‡ºä¸ºONNXæ ¼å¼"""
        self.model.eval()
        torch.onnx.export(
            self.model,
            self.example_input,
            filepath,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={'input': {0: 'batch_size'},
                         'output': {0: 'batch_size'}}
        )
        
        # éªŒè¯ONNXæ¨¡å‹
        onnx_model = onnx.load(filepath)
        onnx.checker.check_model(onnx_model)
        
        return filepath
    
    def create_inference_session(self, onnx_path):
        """åˆ›å»ºæ¨ç†ä¼šè¯"""
        return ort.InferenceSession(onnx_path)
    
    def benchmark_inference(self, onnx_path, test_data, n_runs=100):
        """æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        import time
        
        session = self.create_inference_session(onnx_path)
        
        # é¢„çƒ­
        for _ in range(10):
            session.run(None, {'input': test_data})
        
        # åŸºå‡†æµ‹è¯•
        start_time = time.time()
        for _ in range(n_runs):
            session.run(None, {'input': test_data})
        end_time = time.time()
        
        avg_time = (end_time - start_time) / n_runs
        return avg_time
```

## ğŸ“‹ æœ€ä½³å®è·µæ€»ç»“

### 1. æ•°æ®é¢„å¤„ç†æœ€ä½³å®è·µ
- **ç‰¹å¾ç¼©æ”¾**ï¼šå§‹ç»ˆå¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
- **å¼‚å¸¸å€¼å¤„ç†**ï¼šä½¿ç”¨IQRæ–¹æ³•æˆ–Z-scoreæ£€æµ‹å¼‚å¸¸å€¼
- **ç‰¹å¾å·¥ç¨‹**ï¼šè€ƒè™‘å¤šé¡¹å¼ç‰¹å¾ã€äº¤äº’ç‰¹å¾
- **æ•°æ®éªŒè¯**ï¼šå®æ–½æ•°æ®è´¨é‡æ£€æŸ¥

### 2. æ¨¡å‹è®¾è®¡æœ€ä½³å®è·µ
- **ç½‘ç»œæ·±åº¦**ï¼šä»æµ…å±‚å¼€å§‹ï¼Œé€æ­¥å¢åŠ æ·±åº¦
- **æ­£åˆ™åŒ–**ï¼šç»“åˆDropoutã€æƒé‡è¡°å‡ã€æ‰¹æ ‡å‡†åŒ–
- **æ¿€æ´»å‡½æ•°**ï¼šReLUç³»åˆ—é€šå¸¸æ˜¯å¥½çš„èµ·ç‚¹
- **åˆå§‹åŒ–**ï¼šä½¿ç”¨Xavieræˆ–Heåˆå§‹åŒ–

### 3. è®­ç»ƒæœ€ä½³å®è·µ
- **å­¦ä¹ ç‡**ï¼šä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨ç¡®å®šæœ€ä¼˜å­¦ä¹ ç‡
- **æ‰¹å¤§å°**ï¼šå¹³è¡¡å†…å­˜ä½¿ç”¨å’Œæ¢¯åº¦ç¨³å®šæ€§
- **æ—©åœ**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¿å­˜æœ€ä½³æ¨¡å‹
- **äº¤å‰éªŒè¯**ï¼šç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### 4. è¯„ä¼°æœ€ä½³å®è·µ
- **å¤šæŒ‡æ ‡è¯„ä¼°**ï¼šä¸ä»…ä»…ä¾èµ–å•ä¸€æŒ‡æ ‡
- **æ®‹å·®åˆ†æ**ï¼šæ£€æŸ¥æ¨¡å‹å‡è®¾æ˜¯å¦æ»¡è¶³
- **ç‰¹å¾é‡è¦æ€§**ï¼šç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹
- **æ¨¡å‹è§£é‡Š**ï¼šä½¿ç”¨SHAPç­‰å·¥å…·å¢å¼ºå¯è§£é‡Šæ€§

è¿™ä¸ªä¼˜åŒ–æŒ‡å—ä¸ºæ‚¨çš„MLPæ•°å€¼é¢„æµ‹é¡¹ç›®æä¾›äº†å…¨é¢çš„æ”¹è¿›æ–¹å‘å’Œå…·ä½“å®ç°æ–¹æ¡ˆã€‚æ ¹æ®æ‚¨çš„å…·ä½“éœ€æ±‚å’Œæ•°æ®ç‰¹æ€§ï¼Œå¯ä»¥é€‰æ‹©æ€§åœ°åº”ç”¨è¿™äº›ä¼˜åŒ–ç­–ç•¥ã€‚