# è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•å°†æ‚¨çš„æ•°æ®è½¬æ¢ä¸ºMLPé¡¹ç›®å¯ç”¨çš„æ ¼å¼ï¼ŒåŒ…å«å¤šç§æ•°æ®æºçš„å¤„ç†æ–¹æ³•å’Œå®é™…ç¤ºä¾‹ã€‚

## ğŸ“‹ æ•°æ®æ ¼å¼è¦æ±‚

### åŸºæœ¬è¦æ±‚
- **è¾“å…¥æ•°æ® (X)**: `numpy.ndarray`, å½¢çŠ¶ä¸º `(n_samples, n_features)`, æ•°æ®ç±»å‹ `float32`
- **è¾“å‡ºæ•°æ® (y)**: `numpy.ndarray`, å½¢çŠ¶ä¸º `(n_samples, n_targets)`, æ•°æ®ç±»å‹ `float32`
- **æ•°å€¼èŒƒå›´**: å»ºè®®è¿›è¡Œæ ‡å‡†åŒ–ï¼Œé¡¹ç›®ä¼šè‡ªåŠ¨å¤„ç†
- **ç¼ºå¤±å€¼**: ä¸å…è®¸å­˜åœ¨ `NaN` æˆ– `inf` å€¼

### æ•°æ®ç»´åº¦è¯´æ˜
- `n_samples`: æ ·æœ¬æ•°é‡ (å»ºè®® â‰¥ 100)
- `n_features`: è¾“å…¥ç‰¹å¾æ•°é‡ (æ”¯æŒä»»æ„ç»´åº¦)
- `n_targets`: è¾“å‡ºç›®æ ‡æ•°é‡ (æ”¯æŒå•ç›®æ ‡æˆ–å¤šç›®æ ‡)

## ğŸ”„ æ•°æ®è½¬æ¢æ–¹æ³•

### æ–¹æ³•1: ç›´æ¥ä½¿ç”¨NumPyæ•°ç»„

```python
import numpy as np
from data_processor import DataProcessor

# å‡†å¤‡æ‚¨çš„æ•°æ®
X = your_input_features.astype(np.float32)   # å½¢çŠ¶: (n_samples, n_features)
y = your_target_values.astype(np.float32)    # å½¢çŠ¶: (n_samples, n_targets)

# ç¡®ä¿æ•°æ®å½¢çŠ¶æ­£ç¡®
if X.ndim == 1:
    X = X.reshape(-1, 1)
if y.ndim == 1:
    y = y.reshape(-1, 1)

# ä½¿ç”¨æ•°æ®å¤„ç†å™¨
config = load_config('config.yaml')
processor = DataProcessor(config)
processor.load_data_from_arrays(X, y)
```

### æ–¹æ³•2: ä»Pandas DataFrameè½¬æ¢

```python
import pandas as pd
import numpy as np

# ä»DataFrameæå–æ•°æ®
df = pd.read_csv('your_data.csv')

# å®šä¹‰ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
feature_columns = ['feature1', 'feature2', 'feature3']
target_columns = ['target1', 'target2']

# æå–æ•°æ®
X = df[feature_columns].values.astype(np.float32)
y = df[target_columns].values.astype(np.float32)

# å¤„ç†ç¼ºå¤±å€¼ (å¦‚æœæœ‰)
from sklearn.impute import SimpleImputer
if np.isnan(X).any():
    imputer = SimpleImputer(strategy='mean')
    X = imputer.fit_transform(X)
```

### æ–¹æ³•3: ç›´æ¥ä»CSVæ–‡ä»¶åŠ è½½

```python
from data_processor import DataProcessor

# ä½¿ç”¨å†…ç½®CSVåŠ è½½åŠŸèƒ½
processor = DataProcessor(config)
processor.load_data_from_csv(
    file_path='your_data.csv',
    target_columns=['target1', 'target2'],
    feature_columns=['feature1', 'feature2', 'feature3']  # å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ‰€æœ‰å…¶ä»–åˆ—
)
```

## ğŸ“Š å®é™…æ•°æ®ç¤ºä¾‹

### ç¤ºä¾‹1: æˆ¿ä»·é¢„æµ‹æ•°æ®

```python
# æˆ¿ä»·é¢„æµ‹ç¤ºä¾‹
import numpy as np

# è¾“å…¥ç‰¹å¾: é¢ç§¯, æˆ¿é—´æ•°, æ¥¼å±‚, å»ºé€ å¹´ä»½, è·ç¦»å¸‚ä¸­å¿ƒ
n_samples = 1000
area = np.random.normal(100, 30, n_samples)      # é¢ç§¯ (å¹³æ–¹ç±³)
rooms = np.random.randint(1, 6, n_samples)       # æˆ¿é—´æ•°
floor = np.random.randint(1, 21, n_samples)      # æ¥¼å±‚
year = np.random.randint(1990, 2024, n_samples)  # å»ºé€ å¹´ä»½
distance = np.random.exponential(5, n_samples)   # è·ç¦»å¸‚ä¸­å¿ƒ (å…¬é‡Œ)

X = np.column_stack([area, rooms, floor, year, distance]).astype(np.float32)

# è¾“å‡ºç›®æ ‡: æˆ¿ä»·, ç§Ÿé‡‘
price = (area * 0.8 + rooms * 5 + (2024 - year) * 0.1 - distance * 2 + 
         np.random.normal(0, 5, n_samples))
rent = price * 50 + np.random.normal(0, 200, n_samples)

y = np.column_stack([price, rent]).astype(np.float32)

print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
# è¾“å‡º: æ•°æ®å½¢çŠ¶: X=(1000, 5), y=(1000, 2)
```

### ç¤ºä¾‹2: æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢

```python
# å°†æ—¶é—´åºåˆ—è½¬æ¢ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜
def create_sequences(data, lookback, forecast):
    """
    å°†æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸ºç›‘ç£å­¦ä¹ æ ¼å¼
    
    Args:
        data: æ—¶é—´åºåˆ—æ•°æ® (n_timesteps, n_features)
        lookback: å›çœ‹çª—å£å¤§å°
        forecast: é¢„æµ‹çª—å£å¤§å°
    
    Returns:
        X: (n_samples, lookback * n_features)
        y: (n_samples, forecast * n_targets)
    """
    X, y = [], []
    
    for i in range(lookback, len(data) - forecast):
        # è¾“å…¥: è¿‡å»lookbackä¸ªæ—¶é—´æ­¥çš„æ‰€æœ‰ç‰¹å¾
        x_seq = data[i-lookback:i].flatten()
        
        # è¾“å‡º: æœªæ¥forecastä¸ªæ—¶é—´æ­¥çš„ç›®æ ‡ç‰¹å¾
        y_seq = data[i:i+forecast, :2].flatten()  # å‡è®¾å‰2ä¸ªç‰¹å¾æ˜¯ç›®æ ‡
        
        X.append(x_seq)
        y.append(y_seq)
    
    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)

# ä½¿ç”¨ç¤ºä¾‹
time_series_data = np.random.randn(1000, 4)  # 1000ä¸ªæ—¶é—´æ­¥ï¼Œ4ä¸ªç‰¹å¾
X, y = create_sequences(time_series_data, lookback=24, forecast=6)
print(f"è½¬æ¢åå½¢çŠ¶: X={X.shape}, y={y.shape}")
# è¾“å‡º: è½¬æ¢åå½¢çŠ¶: X=(970, 96), y=(970, 12)
```

### ç¤ºä¾‹3: å›¾åƒç‰¹å¾æ•°æ®

```python
# ä»é¢„è®­ç»ƒæ¨¡å‹æå–çš„å›¾åƒç‰¹å¾
def prepare_image_features(feature_vectors, labels):
    """
    å‡†å¤‡å›¾åƒç‰¹å¾æ•°æ®
    
    Args:
        feature_vectors: å›¾åƒç‰¹å¾å‘é‡åˆ—è¡¨æˆ–æ•°ç»„
        labels: å¯¹åº”çš„æ ‡ç­¾æˆ–è¯„åˆ†
    
    Returns:
        X, y: æ ¼å¼åŒ–åçš„æ•°æ®
    """
    X = np.array(feature_vectors, dtype=np.float32)
    
    # å¦‚æœæ ‡ç­¾æ˜¯åˆ†ç±»ï¼Œè½¬æ¢ä¸ºå›å½’ç›®æ ‡
    if isinstance(labels[0], str):
        # ç¤ºä¾‹: å°†ç±»åˆ«è½¬æ¢ä¸ºæ•°å€¼
        label_map = {'low': 1, 'medium': 5, 'high': 9}
        y = np.array([label_map[label] for label in labels], dtype=np.float32)
        y = y.reshape(-1, 1)
    else:
        y = np.array(labels, dtype=np.float32)
        if y.ndim == 1:
            y = y.reshape(-1, 1)
    
    return X, y

# ä½¿ç”¨ç¤ºä¾‹
features = np.random.randn(500, 512)  # 500å¼ å›¾ç‰‡ï¼Œ512ç»´ç‰¹å¾
scores = np.random.uniform(1, 10, 500)  # è´¨é‡è¯„åˆ† 1-10
X, y = prepare_image_features(features, scores)
```

## ğŸ”§ æ•°æ®é¢„å¤„ç†å»ºè®®

### 1. æ•°æ®æ¸…æ´—

```python
def clean_data(X, y):
    """æ¸…æ´—æ•°æ®ï¼Œç§»é™¤å¼‚å¸¸å€¼å’Œç¼ºå¤±å€¼"""
    # æ£€æŸ¥ç¼ºå¤±å€¼
    if np.isnan(X).any() or np.isnan(y).any():
        print("è­¦å‘Š: å‘ç°ç¼ºå¤±å€¼")
        # ç§»é™¤åŒ…å«ç¼ºå¤±å€¼çš„æ ·æœ¬
        mask = ~(np.isnan(X).any(axis=1) | np.isnan(y).any(axis=1))
        X, y = X[mask], y[mask]
    
    # æ£€æŸ¥æ— ç©·å€¼
    if np.isinf(X).any() or np.isinf(y).any():
        print("è­¦å‘Š: å‘ç°æ— ç©·å€¼")
        mask = ~(np.isinf(X).any(axis=1) | np.isinf(y).any(axis=1))
        X, y = X[mask], y[mask]
    
    # ç§»é™¤å¼‚å¸¸å€¼ (ä½¿ç”¨IQRæ–¹æ³•)
    from scipy import stats
    z_scores = np.abs(stats.zscore(X, axis=0))
    mask = (z_scores < 3).all(axis=1)  # ä¿ç•™z-score < 3çš„æ ·æœ¬
    X, y = X[mask], y[mask]
    
    print(f"æ¸…æ´—åæ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    return X, y
```

### 2. ç‰¹å¾å·¥ç¨‹

```python
def feature_engineering(X):
    """ç‰¹å¾å·¥ç¨‹ç¤ºä¾‹"""
    from sklearn.preprocessing import PolynomialFeatures
    
    # æ·»åŠ å¤šé¡¹å¼ç‰¹å¾
    poly = PolynomialFeatures(degree=2, include_bias=False)
    X_poly = poly.fit_transform(X)
    
    # æ·»åŠ ç»Ÿè®¡ç‰¹å¾
    X_stats = np.column_stack([
        X.mean(axis=1),  # æ¯ä¸ªæ ·æœ¬çš„å‡å€¼
        X.std(axis=1),   # æ¯ä¸ªæ ·æœ¬çš„æ ‡å‡†å·®
        X.max(axis=1),   # æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§å€¼
        X.min(axis=1)    # æ¯ä¸ªæ ·æœ¬çš„æœ€å°å€¼
    ])
    
    # åˆå¹¶ç‰¹å¾
    X_enhanced = np.column_stack([X, X_stats])
    
    return X_enhanced.astype(np.float32)
```

### 3. æ•°æ®éªŒè¯

```python
def validate_data(X, y):
    """éªŒè¯æ•°æ®æ ¼å¼å’Œè´¨é‡"""
    checks = []
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    checks.append(("Xæ•°æ®ç±»å‹", X.dtype == np.float32))
    checks.append(("yæ•°æ®ç±»å‹", y.dtype == np.float32))
    
    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
    checks.append(("Xæ˜¯2Dæ•°ç»„", X.ndim == 2))
    checks.append(("yæ˜¯2Dæ•°ç»„", y.ndim == 2))
    checks.append(("æ ·æœ¬æ•°é‡åŒ¹é…", X.shape[0] == y.shape[0]))
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    checks.append(("Xæ— ç¼ºå¤±å€¼", not np.isnan(X).any()))
    checks.append(("yæ— ç¼ºå¤±å€¼", not np.isnan(y).any()))
    checks.append(("Xæ— æ— ç©·å€¼", not np.isinf(X).any()))
    checks.append(("yæ— æ— ç©·å€¼", not np.isinf(y).any()))
    
    # æ£€æŸ¥æ ·æœ¬æ•°é‡
    checks.append(("æ ·æœ¬æ•°é‡å……è¶³", X.shape[0] >= 100))
    
    print("æ•°æ®éªŒè¯ç»“æœ:")
    for check_name, result in checks:
        status = "âœ…" if result else "âŒ"
        print(f"  {status} {check_name}")
    
    all_passed = all(result for _, result in checks)
    
    if all_passed:
        print("ğŸ‰ æ•°æ®éªŒè¯é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
    else:
        print("âš ï¸  æ•°æ®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¹¶ä¿®å¤é—®é¢˜ã€‚")
    
    return all_passed
```

## ğŸ“ å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
# å®Œæ•´çš„æ•°æ®å‡†å¤‡å’Œè®­ç»ƒæµç¨‹
import numpy as np
import yaml
from data_processor import DataProcessor
from mlp_model import create_model_from_config
from trainer import MLPTrainer

def train_with_your_data():
    """ä½¿ç”¨æ‚¨çš„æ•°æ®è®­ç»ƒæ¨¡å‹çš„å®Œæ•´æµç¨‹"""
    
    # æ­¥éª¤1: å‡†å¤‡æ•°æ®
    # æ›¿æ¢ä¸ºæ‚¨çš„å®é™…æ•°æ®åŠ è½½ä»£ç 
    X = np.random.randn(1000, 10).astype(np.float32)  # æ‚¨çš„è¾“å…¥ç‰¹å¾
    y = np.random.randn(1000, 3).astype(np.float32)   # æ‚¨çš„ç›®æ ‡å€¼
    
    # æ­¥éª¤2: æ•°æ®éªŒè¯
    if not validate_data(X, y):
        return False
    
    # æ­¥éª¤3: æ•°æ®æ¸…æ´— (å¯é€‰)
    X, y = clean_data(X, y)
    
    # æ­¥éª¤4: ç‰¹å¾å·¥ç¨‹ (å¯é€‰)
    # X = feature_engineering(X)
    
    # æ­¥éª¤5: åŠ è½½é…ç½®
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # æ­¥éª¤6: åˆ›å»ºæ•°æ®å¤„ç†å™¨
    processor = DataProcessor(config)
    processor.load_data_from_arrays(X, y)
    processor.normalize_data()
    
    # æ­¥éª¤7: åˆ†å‰²æ•°æ®
    X_train, X_val, X_test, y_train, y_val, y_test = processor.split_data()
    train_loader, val_loader, test_loader = processor.create_data_loaders(
        X_train, X_val, X_test, y_train, y_val, y_test
    )
    
    # æ­¥éª¤8: åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
    model = create_model_from_config(config, processor.input_dim, processor.output_dim)
    trainer = MLPTrainer(model, config)
    
    print("å¼€å§‹è®­ç»ƒ...")
    history = trainer.train(train_loader, val_loader)
    
    # æ­¥éª¤9: è¯„ä¼°æ¨¡å‹
    test_pred, test_true = trainer.predict(test_loader)
    test_pred_orig = processor.inverse_transform_predictions(test_pred)
    test_true_orig = processor.inverse_transform_predictions(test_true)
    
    from evaluator import ModelEvaluator
    evaluator = ModelEvaluator()
    metrics = evaluator.evaluate_model(test_true_orig, test_pred_orig)
    
    print(f"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆRÂ²åˆ†æ•°: {metrics['r2']:.4f}")
    
    return True

# è¿è¡Œè®­ç»ƒ
if __name__ == "__main__":
    train_with_your_data()
```

## ğŸš¨ NaNå€¼å¤„ç†æŒ‡å—

### NaNå€¼çš„è¯†åˆ«å’Œåˆ†æ

å¦‚æœæ‚¨çš„æ•°æ®ä¸­å­˜åœ¨NaNå€¼ï¼Œéœ€è¦åœ¨è®­ç»ƒå‰è¿›è¡Œå¤„ç†ã€‚é¡¹ç›®æä¾›äº†ä¸“é—¨çš„NaNå¤„ç†å·¥å…·ï¼š

```python
from nan_handling_guide import NaNHandler

# åˆ›å»ºNaNå¤„ç†å™¨
handler = NaNHandler()

# åˆ†æNaNåˆ†å¸ƒæ¨¡å¼
analysis = handler.analyze_nan_pattern(X, y, feature_names=['ç‰¹å¾1', 'ç‰¹å¾2', ...])
handler.print_nan_summary(analysis)

# å¯è§†åŒ–NaNæ¨¡å¼
handler.visualize_nan_pattern(X, save_path="nan_analysis.png")
```

### NaNå¤„ç†ç­–ç•¥

#### ç­–ç•¥1: åˆ é™¤å«NaNçš„æ ·æœ¬
```python
# é€‚ç”¨äº: å®Œæ•´æ ·æœ¬æ¯”ä¾‹è¾ƒé«˜(>70%)çš„æƒ…å†µ
X_clean, y_clean = handler.strategy_1_remove_samples(X, y, threshold=0.5)
```

#### ç­–ç•¥2: åˆ é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾
```python
# é€‚ç”¨äº: æŸäº›ç‰¹å¾ç¼ºå¤±ç‡è¿‡é«˜(>70%)çš„æƒ…å†µ
X_clean, y_clean, remaining_features = handler.strategy_2_remove_features(
    X, y, threshold=0.7, feature_names=feature_names
)
```

#### ç­–ç•¥3: ç®€å•æ’å€¼å¡«å……
```python
# é€‚ç”¨äº: ç¼ºå¤±å€¼è¾ƒå°‘ä¸”éšæœºåˆ†å¸ƒçš„æƒ…å†µ
X_clean, y_clean = handler.strategy_3_simple_imputation(X, y, strategy='mean')
# strategyå¯é€‰: 'mean', 'median', 'most_frequent'
```

#### ç­–ç•¥4: KNNæ’å€¼å¡«å……
```python
# é€‚ç”¨äº: ç‰¹å¾é—´æœ‰ç›¸å…³æ€§ï¼Œç¼ºå¤±å€¼é€‚ä¸­çš„æƒ…å†µ
X_clean, y_clean = handler.strategy_4_knn_imputation(X, y, n_neighbors=5)
```

#### ç­–ç•¥5: è¿­ä»£æ’å€¼å¡«å……(MICE)
```python
# é€‚ç”¨äº: å¤æ‚ç¼ºå¤±æ¨¡å¼ï¼Œæ ·æœ¬é‡å……è¶³çš„æƒ…å†µ
X_clean, y_clean = handler.strategy_5_iterative_imputation(X, y, max_iter=10)
```

#### ç­–ç•¥6: æ··åˆæ–¹æ³•(æ¨è)
```python
# ç»¼åˆå¤šç§æ–¹æ³•ï¼Œé€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µ
X_clean, y_clean = handler.strategy_6_hybrid_approach(X, y)
```

### è‡ªåŠ¨ç­–ç•¥æ¨è

```python
# è·å–åŸºäºæ•°æ®ç‰¹æ€§çš„ç­–ç•¥æ¨è
recommended_strategy = handler.recommend_strategy(analysis)
print(f"æ¨èç­–ç•¥: {recommended_strategy}")

# æ¯”è¾ƒä¸åŒç­–ç•¥çš„æ•ˆæœ
comparison = handler.compare_strategies(X, y)
for strategy, result in comparison.items():
    print(f"{strategy}: ä¿ç•™æ ·æœ¬{result['samples_retained']*100:.1f}%")
```

### å®Œæ•´çš„NaNå¤„ç†æµç¨‹

```python
def handle_nan_data(X, y):
    """å¤„ç†å«NaNæ•°æ®çš„å®Œæ•´æµç¨‹"""
    
    # 1. åˆ†æNaNæ¨¡å¼
    handler = NaNHandler()
    analysis = handler.analyze_nan_pattern(X, y)
    handler.print_nan_summary(analysis)
    
    # 2. è·å–æ¨èç­–ç•¥
    strategy = handler.recommend_strategy(analysis)
    
    # 3. åº”ç”¨å¤„ç†ç­–ç•¥
    if strategy == 'hybrid':
        X_clean, y_clean = handler.strategy_6_hybrid_approach(X, y)
    elif strategy == 'knn':
        X_clean, y_clean = handler.strategy_4_knn_imputation(X, y)
    # ... å…¶ä»–ç­–ç•¥
    
    # 4. éªŒè¯å¤„ç†ç»“æœ
    assert not np.isnan(X_clean).any(), "ä»æœ‰NaNå€¼"
    assert not np.isnan(y_clean).any(), "ç›®æ ‡å€¼ä»æœ‰NaN"
    
    print(f"NaNå¤„ç†å®Œæˆ: {X.shape} -> {X_clean.shape}")
    return X_clean, y_clean

# ä½¿ç”¨ç¤ºä¾‹
X_clean, y_clean = handle_nan_data(your_X_with_nan, your_y_with_nan)

# ç»§ç»­æ­£å¸¸çš„è®­ç»ƒæµç¨‹
processor = DataProcessor(config)
processor.load_data_from_arrays(X_clean, y_clean)
```

### NaNå¤„ç†æœ€ä½³å®è·µ

1. **å…ˆåˆ†æåå¤„ç†**: äº†è§£NaNçš„åˆ†å¸ƒæ¨¡å¼å†é€‰æ‹©ç­–ç•¥
2. **ä¿ç•™è¶³å¤Ÿæ•°æ®**: é¿å…è¿‡åº¦åˆ é™¤å¯¼è‡´æ•°æ®ä¸è¶³
3. **éªŒè¯å¤„ç†æ•ˆæœ**: ç¡®ä¿å¤„ç†åæ•°æ®è´¨é‡è‰¯å¥½
4. **è®°å½•å¤„ç†è¿‡ç¨‹**: ä¾¿äºåç»­æ•°æ®é¢„å¤„ç†çš„ä¸€è‡´æ€§
5. **è€ƒè™‘ä¸šåŠ¡å«ä¹‰**: NaNå¯èƒ½æœ‰ç‰¹æ®Šå«ä¹‰ï¼Œä¸ä¸€å®šè¦å¡«å……

## âš ï¸ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: æ•°æ®ç±»å‹é”™è¯¯
```
TypeError: can't convert np.ndarray of type numpy.object_
```
**è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿æ•°æ®ç±»å‹ä¸ºæ•°å€¼å‹
```python
X = X.astype(np.float32)
y = y.astype(np.float32)
```

### é—®é¢˜2: æ•°æ®ç»´åº¦é”™è¯¯
```
ValueError: Expected 2D array, got 1D array
```
**è§£å†³æ–¹æ¡ˆ**: è°ƒæ•´æ•°æ®ç»´åº¦
```python
if X.ndim == 1:
    X = X.reshape(-1, 1)
if y.ndim == 1:
    y = y.reshape(-1, 1)
```

### é—®é¢˜3: æ ·æœ¬æ•°é‡ä¸åŒ¹é…
```
ValueError: X and y must have the same number of samples
```
**è§£å†³æ–¹æ¡ˆ**: æ£€æŸ¥å¹¶å¯¹é½æ ·æœ¬æ•°é‡
```python
min_samples = min(len(X), len(y))
X = X[:min_samples]
y = y[:min_samples]
```

### é—®é¢˜4: å†…å­˜ä¸è¶³
```
RuntimeError: CUDA out of memory
```
**è§£å†³æ–¹æ¡ˆ**: å‡å°æ‰¹æ¬¡å¤§å°æˆ–æ•°æ®é‡
```python
config['training']['batch_size'] = 16  # å‡å°æ‰¹æ¬¡å¤§å°
# æˆ–è€…ä½¿ç”¨æ•°æ®å­é›†
X = X[:5000]  # åªä½¿ç”¨å‰5000ä¸ªæ ·æœ¬
y = y[:5000]
```

## ğŸ¯ æœ€ä½³å®è·µå»ºè®®

1. **æ•°æ®è´¨é‡**: ç¡®ä¿æ•°æ®æ— ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼å’Œé‡å¤æ ·æœ¬
2. **ç‰¹å¾ç¼©æ”¾**: è®©é¡¹ç›®è‡ªåŠ¨å¤„ç†æ ‡å‡†åŒ–ï¼Œæˆ–æ‰‹åŠ¨è¿›è¡Œç‰¹å¾ç¼©æ”¾
3. **æ•°æ®åˆ†å¸ƒ**: æ£€æŸ¥ç›®æ ‡å˜é‡çš„åˆ†å¸ƒï¼Œè€ƒè™‘æ˜¯å¦éœ€è¦å˜æ¢
4. **æ ·æœ¬å¹³è¡¡**: ç¡®ä¿è®­ç»ƒé›†ä¸­å„ç±»æ ·æœ¬åˆ†å¸ƒåˆç†
5. **éªŒè¯é›†**: ä¿ç•™è¶³å¤Ÿçš„éªŒè¯æ•°æ®ç”¨äºæ¨¡å‹é€‰æ‹©
6. **æ–‡æ¡£è®°å½•**: è®°å½•æ•°æ®æ¥æºã€é¢„å¤„ç†æ­¥éª¤å’Œç‰¹å¾å«ä¹‰

é€šè¿‡éµå¾ªè¿™ä¸ªæŒ‡å—ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°†ä»»ä½•æ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºé¡¹ç›®å¯ç”¨çš„æ ¼å¼ï¼Œå¹¶è·å¾—è‰¯å¥½çš„è®­ç»ƒæ•ˆæœï¼