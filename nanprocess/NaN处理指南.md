# NaNå€¼å¤„ç†å®Œæ•´æŒ‡å—

åœ¨æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­ï¼Œç¼ºå¤±å€¼ï¼ˆNaNï¼‰æ˜¯ä¸€ä¸ªå¸¸è§ä¸”é‡è¦çš„æ•°æ®è´¨é‡é—®é¢˜ã€‚æœ¬æŒ‡å—æä¾›äº†å…¨é¢çš„NaNå€¼å¤„ç†ç­–ç•¥å’Œå®ç”¨å»ºè®®ã€‚

## ğŸ“‹ ç›®å½•

- [NaNå€¼æ¦‚è¿°](#nanå€¼æ¦‚è¿°)
- [NaNå€¼çš„è¯†åˆ«ä¸åˆ†æ](#nanå€¼çš„è¯†åˆ«ä¸åˆ†æ)
- [å¤„ç†ç­–ç•¥è¯¦è§£](#å¤„ç†ç­–ç•¥è¯¦è§£)
- [ç­–ç•¥é€‰æ‹©æŒ‡å—](#ç­–ç•¥é€‰æ‹©æŒ‡å—)
- [å®é™…åº”ç”¨ç¤ºä¾‹](#å®é™…åº”ç”¨ç¤ºä¾‹)
- [æœ€ä½³å®è·µå»ºè®®](#æœ€ä½³å®è·µå»ºè®®)
- [å¸¸è§é—®é¢˜è§£ç­”](#å¸¸è§é—®é¢˜è§£ç­”)

## ğŸ” NaNå€¼æ¦‚è¿°

### ä»€ä¹ˆæ˜¯NaNå€¼ï¼Ÿ
NaNï¼ˆNot a Numberï¼‰è¡¨ç¤ºç¼ºå¤±æˆ–æœªå®šä¹‰çš„æ•°å€¼ã€‚åœ¨æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­ï¼Œç”±äºå„ç§åŸå› å¯èƒ½äº§ç”Ÿç¼ºå¤±å€¼ï¼š

- **è®¾å¤‡æ•…éšœ**ï¼šä¼ æ„Ÿå™¨æ•…éšœå¯¼è‡´æ•°æ®æœªè®°å½•
- **äººä¸ºé”™è¯¯**ï¼šæ•°æ®å½•å…¥æ—¶çš„é—æ¼æˆ–é”™è¯¯
- **ç³»ç»Ÿé—®é¢˜**ï¼šæ•°æ®ä¼ è¾“ä¸­æ–­æˆ–å­˜å‚¨é”™è¯¯
- **éšç§ä¿æŠ¤**ï¼šæ•æ„Ÿä¿¡æ¯è¢«æ•…æ„éšè—
- **æ•°æ®åˆå¹¶**ï¼šä¸åŒæ•°æ®æºåˆå¹¶æ—¶çš„ä¸åŒ¹é…

### NaNå€¼çš„å½±å“
- **è®­ç»ƒå¤±è´¥**ï¼šå¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•æ— æ³•å¤„ç†NaNå€¼
- **ç»“æœåå·®**ï¼šä¸å½“å¤„ç†å¯èƒ½å¼•å…¥åå·®
- **æ€§èƒ½ä¸‹é™**ï¼šæ•°æ®è´¨é‡é—®é¢˜å½±å“æ¨¡å‹æ€§èƒ½
- **è®¡ç®—é”™è¯¯**ï¼šNaNå€¼ä¼šä¼ æ’­åˆ°è®¡ç®—ç»“æœä¸­

## ğŸ” NaNå€¼çš„è¯†åˆ«ä¸åˆ†æ

### å¿«é€Ÿæ£€æµ‹
```python
import numpy as np
import pandas as pd

# æ£€æŸ¥æ˜¯å¦å­˜åœ¨NaN
has_nan_X = np.isnan(X).any()
has_nan_y = np.isnan(y).any()

# ç»Ÿè®¡NaNæ•°é‡
nan_count_X = np.isnan(X).sum()
nan_count_y = np.isnan(y).sum()

# è®¡ç®—NaNæ¯”ä¾‹
nan_ratio_X = np.isnan(X).sum() / X.size
nan_ratio_y = np.isnan(y).sum() / y.size

print(f"è¾“å…¥ç‰¹å¾NaNæ¯”ä¾‹: {nan_ratio_X*100:.2f}%")
print(f"ç›®æ ‡å€¼NaNæ¯”ä¾‹: {nan_ratio_y*100:.2f}%")
```

### è¯¦ç»†åˆ†æå·¥å…·
```python
from nan_handling_guide import NaNHandler

# åˆ›å»ºåˆ†æå™¨
handler = NaNHandler()

# å…¨é¢åˆ†æNaNæ¨¡å¼
analysis = handler.analyze_nan_pattern(X, y, feature_names=['ç‰¹å¾1', 'ç‰¹å¾2', ...])

# æ‰“å°åˆ†ææŠ¥å‘Š
handler.print_nan_summary(analysis)

# å¯è§†åŒ–NaNåˆ†å¸ƒ
handler.visualize_nan_pattern(X, save_path="nan_analysis.png")
```

### åˆ†ææŠ¥å‘Šè§£è¯»
åˆ†ææŠ¥å‘ŠåŒ…å«ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š
- **æ€»ä½“æ¦‚å†µ**ï¼šæ ·æœ¬æ•°ã€ç‰¹å¾æ•°ã€å®Œæ•´æ ·æœ¬æ¯”ä¾‹
- **ç‰¹å¾çº§åˆ«**ï¼šæ¯ä¸ªç‰¹å¾çš„ç¼ºå¤±æ•°é‡å’Œæ¯”ä¾‹
- **æ ·æœ¬çº§åˆ«**ï¼šæ¯ä¸ªæ ·æœ¬çš„ç¼ºå¤±ç‰¹å¾æ•°é‡
- **æ¨¡å¼è¯†åˆ«**ï¼šé«˜ç¼ºå¤±ç‡ç‰¹å¾ã€å®Œå…¨ç¼ºå¤±æ ·æœ¬ç­‰

## ğŸ› ï¸ å¤„ç†ç­–ç•¥è¯¦è§£

### ç­–ç•¥1ï¼šåˆ é™¤å«NaNçš„æ ·æœ¬

**é€‚ç”¨åœºæ™¯**ï¼š
- å®Œæ•´æ ·æœ¬æ¯”ä¾‹è¾ƒé«˜ï¼ˆ>70%ï¼‰
- æ ·æœ¬é‡å……è¶³
- NaNåˆ†å¸ƒç›¸å¯¹éšæœº

**ä¼˜ç‚¹**ï¼š
- ç®€å•ç›´æ¥ï¼Œä¸å¼•å…¥ä¼°è®¡è¯¯å·®
- ä¿æŒæ•°æ®çš„çœŸå®æ€§
- è®¡ç®—æ•ˆç‡é«˜

**ç¼ºç‚¹**ï¼š
- å¯èƒ½ä¸¢å¤±å¤§é‡æ•°æ®
- å¦‚æœNaNä¸æ˜¯éšæœºåˆ†å¸ƒï¼Œå¯èƒ½å¼•å…¥åå·®

**å®ç°æ–¹æ³•**ï¼š
```python
# åˆ é™¤å«æœ‰è¶…è¿‡50%ç‰¹å¾ç¼ºå¤±çš„æ ·æœ¬
X_clean, y_clean = handler.strategy_1_remove_samples(X, y, threshold=0.5)

# æˆ–è€…åˆ é™¤ä»»ä½•å«æœ‰NaNçš„æ ·æœ¬
mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y).any()
X_clean, y_clean = X[mask], y[mask]
```

**ä½¿ç”¨å»ºè®®**ï¼š
- å½“å®Œæ•´æ ·æœ¬>70%æ—¶ä¼˜å…ˆè€ƒè™‘
- è®¾ç½®åˆç†çš„ç¼ºå¤±é˜ˆå€¼ï¼ˆå»ºè®®0.3-0.5ï¼‰
- æ£€æŸ¥åˆ é™¤åçš„æ•°æ®åˆ†å¸ƒæ˜¯å¦å‘ç”Ÿæ˜¾è‘—å˜åŒ–

### ç­–ç•¥2ï¼šåˆ é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾

**é€‚ç”¨åœºæ™¯**ï¼š
- æŸäº›ç‰¹å¾ç¼ºå¤±ç‡æé«˜ï¼ˆ>70%ï¼‰
- ç‰¹å¾æ•°é‡è¾ƒå¤š
- é«˜ç¼ºå¤±ç‰¹å¾å¯¹ç›®æ ‡å˜é‡è´¡çŒ®è¾ƒå°

**ä¼˜ç‚¹**ï¼š
- ä¿ç•™æ›´å¤šæ ·æœ¬
- å»é™¤ä½è´¨é‡ç‰¹å¾
- å‡å°‘ç‰¹å¾ç»´åº¦

**ç¼ºç‚¹**ï¼š
- å¯èƒ½ä¸¢å¤±é‡è¦ä¿¡æ¯
- éœ€è¦é¢†åŸŸçŸ¥è¯†åˆ¤æ–­ç‰¹å¾é‡è¦æ€§

**å®ç°æ–¹æ³•**ï¼š
```python
# åˆ é™¤ç¼ºå¤±ç‡è¶…è¿‡70%çš„ç‰¹å¾
X_clean, y_clean, remaining_features = handler.strategy_2_remove_features(
    X, y, threshold=0.7, feature_names=feature_names
)

print(f"åˆ é™¤äº† {len(feature_names) - len(remaining_features)} ä¸ªé«˜ç¼ºå¤±ç‰¹å¾")
```

**ä½¿ç”¨å»ºè®®**ï¼š
- ç»“åˆä¸šåŠ¡çŸ¥è¯†è¯„ä¼°ç‰¹å¾é‡è¦æ€§
- å¯ä»¥å…ˆå°è¯•è¾ƒé«˜é˜ˆå€¼ï¼ˆ0.8ï¼‰ï¼Œå†é€æ­¥é™ä½
- ä¿ç•™çš„ç‰¹å¾æ•°é‡åº”è¶³å¤Ÿè¿›è¡Œæœ‰æ•ˆå»ºæ¨¡

### ç­–ç•¥3ï¼šç®€å•æ’å€¼å¡«å……

**é€‚ç”¨åœºæ™¯**ï¼š
- NaNæ¯”ä¾‹è¾ƒä½ï¼ˆ<20%ï¼‰
- ç‰¹å¾åˆ†å¸ƒç›¸å¯¹ç¨³å®š
- éœ€è¦å¿«é€Ÿå¤„ç†

**å¸¸ç”¨æ–¹æ³•**ï¼š

#### å‡å€¼å¡«å……
```python
# ä½¿ç”¨å‡å€¼å¡«å……
X_clean, y_clean = handler.strategy_3_simple_imputation(X, y, strategy='mean')
```
- **é€‚ç”¨**ï¼šæ•°å€¼ç‰¹å¾ï¼Œæ­£æ€åˆ†å¸ƒ
- **ä¼˜ç‚¹**ï¼šç®€å•å¿«é€Ÿï¼Œä¸æ”¹å˜å‡å€¼
- **ç¼ºç‚¹**ï¼šå‡å°‘æ–¹å·®ï¼Œå¯èƒ½å¼•å…¥åå·®

#### ä¸­ä½æ•°å¡«å……
```python
# ä½¿ç”¨ä¸­ä½æ•°å¡«å……
X_clean, y_clean = handler.strategy_3_simple_imputation(X, y, strategy='median')
```
- **é€‚ç”¨**ï¼šæœ‰å¼‚å¸¸å€¼çš„æ•°å€¼ç‰¹å¾
- **ä¼˜ç‚¹**ï¼šå¯¹å¼‚å¸¸å€¼é²æ£’
- **ç¼ºç‚¹**ï¼šåŒæ ·å‡å°‘æ–¹å·®

#### ä¼—æ•°å¡«å……
```python
# ä½¿ç”¨ä¼—æ•°å¡«å……
X_clean, y_clean = handler.strategy_3_simple_imputation(X, y, strategy='most_frequent')
```
- **é€‚ç”¨**ï¼šåˆ†ç±»ç‰¹å¾
- **ä¼˜ç‚¹**ï¼šä¿æŒåˆ†ç±»åˆ†å¸ƒ
- **ç¼ºç‚¹**ï¼šå¯èƒ½è¿‡åº¦é›†ä¸­äºæŸä¸ªç±»åˆ«

**ä½¿ç”¨å»ºè®®**ï¼š
- æ•°å€¼ç‰¹å¾ä¼˜å…ˆä½¿ç”¨ä¸­ä½æ•°
- åˆ†ç±»ç‰¹å¾ä½¿ç”¨ä¼—æ•°
- æ£€æŸ¥å¡«å……åçš„åˆ†å¸ƒå˜åŒ–

### ç­–ç•¥4ï¼šKNNæ’å€¼å¡«å……

**é€‚ç”¨åœºæ™¯**ï¼š
- ç‰¹å¾é—´å­˜åœ¨ç›¸å…³æ€§
- NaNæ¯”ä¾‹é€‚ä¸­ï¼ˆ20%-50%ï¼‰
- æ•°æ®è´¨é‡è¾ƒå¥½

**å·¥ä½œåŸç†**ï¼š
1. å¯¹æ¯ä¸ªå«NaNçš„æ ·æœ¬ï¼Œæ‰¾åˆ°Kä¸ªæœ€ç›¸ä¼¼çš„å®Œæ•´æ ·æœ¬
2. ä½¿ç”¨è¿™Kä¸ªæ ·æœ¬çš„å‡å€¼æ¥å¡«å……NaNå€¼
3. ç›¸ä¼¼æ€§é€šå¸¸åŸºäºæ¬§æ°è·ç¦»è®¡ç®—

**å®ç°æ–¹æ³•**ï¼š
```python
# KNNæ’å€¼ï¼Œä½¿ç”¨5ä¸ªæœ€è¿‘é‚»
X_clean, y_clean = handler.strategy_4_knn_imputation(X, y, n_neighbors=5)
```

**å‚æ•°è°ƒä¼˜**ï¼š
- **n_neighbors**ï¼šé‚»å±…æ•°é‡
  - å°å€¼ï¼ˆ3-5ï¼‰ï¼šæ›´ç²¾ç¡®ä½†å¯èƒ½è¿‡æ‹Ÿåˆ
  - å¤§å€¼ï¼ˆ10-20ï¼‰ï¼šæ›´ç¨³å®šä½†å¯èƒ½è¿‡äºå¹³æ»‘
- **è·ç¦»åº¦é‡**ï¼šé»˜è®¤æ¬§æ°è·ç¦»ï¼Œå¯è€ƒè™‘æ›¼å“ˆé¡¿è·ç¦»

**ä¼˜ç‚¹**ï¼š
- è€ƒè™‘ç‰¹å¾é—´å…³ç³»
- å¡«å……å€¼æ›´åˆç†
- ä¿æŒæ•°æ®åˆ†å¸ƒ

**ç¼ºç‚¹**ï¼š
- è®¡ç®—å¤æ‚åº¦é«˜
- å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
- éœ€è¦è¶³å¤Ÿçš„å®Œæ•´æ ·æœ¬

**ä½¿ç”¨å»ºè®®**ï¼š
- æ•°æ®æ ‡å‡†åŒ–åä½¿ç”¨
- æ ¹æ®æ•°æ®é‡è°ƒæ•´é‚»å±…æ•°
- é€‚åˆä¸­ç­‰è§„æ¨¡æ•°æ®é›†

### ç­–ç•¥5ï¼šè¿­ä»£æ’å€¼å¡«å……ï¼ˆMICEï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤æ‚çš„ç¼ºå¤±æ¨¡å¼
- ç‰¹å¾é—´æœ‰å¼ºç›¸å…³æ€§
- æ ·æœ¬é‡å……è¶³ï¼ˆ>1000ï¼‰
- å¯¹ç²¾åº¦è¦æ±‚é«˜

**å·¥ä½œåŸç†**ï¼š
1. åˆå§‹åŒ–ï¼šç”¨ç®€å•æ–¹æ³•å¡«å……æ‰€æœ‰NaN
2. è¿­ä»£ï¼šä¾æ¬¡å¯¹æ¯ä¸ªç‰¹å¾å»ºç«‹å›å½’æ¨¡å‹
3. é¢„æµ‹ï¼šç”¨å…¶ä»–ç‰¹å¾é¢„æµ‹å½“å‰ç‰¹å¾çš„ç¼ºå¤±å€¼
4. æ›´æ–°ï¼šç”¨é¢„æµ‹å€¼æ›¿æ¢ç¼ºå¤±å€¼
5. é‡å¤ï¼šç›´åˆ°æ”¶æ•›æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°

**å®ç°æ–¹æ³•**ï¼š
```python
# è¿­ä»£æ’å€¼ï¼Œæœ€å¤š10æ¬¡è¿­ä»£
X_clean, y_clean = handler.strategy_5_iterative_imputation(X, y, max_iter=10)
```

**ä¼˜ç‚¹**ï¼š
- æœ€ç²¾ç¡®çš„æ’å€¼æ–¹æ³•
- è€ƒè™‘æ‰€æœ‰ç‰¹å¾é—´å…³ç³»
- ç†è®ºåŸºç¡€æ‰å®

**ç¼ºç‚¹**ï¼š
- è®¡ç®—æ—¶é—´é•¿
- å¯èƒ½ä¸æ”¶æ•›
- å¯¹åˆå§‹å€¼æ•æ„Ÿ

**ä½¿ç”¨å»ºè®®**ï¼š
- å¤§æ•°æ®é›†ä¼˜å…ˆè€ƒè™‘
- ç›‘æ§æ”¶æ•›æƒ…å†µ
- å¯ä»¥å…ˆç”¨å…¶ä»–æ–¹æ³•é¢„å¤„ç†

### ç­–ç•¥6ï¼šæ··åˆæ–¹æ³•ï¼ˆæ¨èï¼‰

**è®¾è®¡æ€è·¯**ï¼š
ç»“åˆå¤šç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œåˆ†æ­¥éª¤å¤„ç†ä¸åŒç¨‹åº¦çš„ç¼ºå¤±é—®é¢˜ã€‚

**å¤„ç†æµç¨‹**ï¼š
1. **ç¬¬ä¸€æ­¥**ï¼šåˆ é™¤ç¼ºå¤±ç‡>70%çš„ç‰¹å¾
2. **ç¬¬äºŒæ­¥**ï¼šåˆ é™¤ç¼ºå¤±ç‡>50%çš„æ ·æœ¬
3. **ç¬¬ä¸‰æ­¥**ï¼šå¯¹å‰©ä½™NaNä½¿ç”¨KNNæ’å€¼

**å®ç°æ–¹æ³•**ï¼š
```python
# æ··åˆæ–¹æ³•ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³å‚æ•°
X_clean, y_clean = handler.strategy_6_hybrid_approach(X, y)

# è‡ªå®šä¹‰å‚æ•°
X_clean, y_clean = handler.strategy_6_hybrid_approach(
    X, y, 
    high_missing_threshold=0.7,    # ç‰¹å¾åˆ é™¤é˜ˆå€¼
    sample_missing_threshold=0.5   # æ ·æœ¬åˆ é™¤é˜ˆå€¼
)
```

**ä¼˜ç‚¹**ï¼š
- å¹³è¡¡æ•°æ®ä¿ç•™å’Œè´¨é‡
- é€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯
- è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜

**ç¼ºç‚¹**ï¼š
- å‚æ•°éœ€è¦è°ƒä¼˜
- å¯èƒ½è¿‡äºå¤æ‚

**ä½¿ç”¨å»ºè®®**ï¼š
- ä½œä¸ºé»˜è®¤é€‰æ‹©
- æ ¹æ®æ•°æ®ç‰¹ç‚¹è°ƒæ•´é˜ˆå€¼
- ç›‘æ§æ¯æ­¥çš„æ•°æ®æŸå¤±

## ğŸ¯ ç­–ç•¥é€‰æ‹©æŒ‡å—

### è‡ªåŠ¨æ¨èç³»ç»Ÿ

é¡¹ç›®æä¾›æ™ºèƒ½æ¨èåŠŸèƒ½ï¼š

```python
# è·å–æ¨èç­–ç•¥
recommended_strategy = handler.recommend_strategy(analysis)
print(f"æ¨èç­–ç•¥: {recommended_strategy}")

# æ¯”è¾ƒæ‰€æœ‰ç­–ç•¥æ•ˆæœ
comparison = handler.compare_strategies(X, y)
for strategy, result in comparison.items():
    print(f"{strategy}: ä¿ç•™{result['samples_retained']*100:.1f}%æ ·æœ¬")
```

### å†³ç­–æ ‘æŒ‡å—

```
å¼€å§‹
â”œâ”€â”€ å®Œæ•´æ ·æœ¬æ¯”ä¾‹ > 70%ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ ç­–ç•¥1ï¼šåˆ é™¤å«NaNæ ·æœ¬
â”‚   â””â”€â”€ å¦ â†“
â”œâ”€â”€ å­˜åœ¨é«˜ç¼ºå¤±ç‡ç‰¹å¾ï¼ˆ>70%ï¼‰ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ ç­–ç•¥6ï¼šæ··åˆæ–¹æ³•
â”‚   â””â”€â”€ å¦ â†“
â”œâ”€â”€ æ ·æœ¬é‡ > 1000 ä¸” NaNæ¯”ä¾‹ < 30%ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ ç­–ç•¥5ï¼šè¿­ä»£æ’å€¼
â”‚   â””â”€â”€ å¦ â†“
â”œâ”€â”€ ç‰¹å¾é—´æœ‰ç›¸å…³æ€§ ä¸” NaNæ¯”ä¾‹ < 50%ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ ç­–ç•¥4ï¼šKNNæ’å€¼
â”‚   â””â”€â”€ å¦ â†“
â””â”€â”€ é»˜è®¤ â†’ ç­–ç•¥3ï¼šç®€å•æ’å€¼
```

### åœºæ™¯åŒ–å»ºè®®

#### åœºæ™¯1ï¼šä¼ æ„Ÿå™¨æ•°æ®
- **ç‰¹ç‚¹**ï¼šæ—¶é—´åºåˆ—ï¼Œéƒ¨åˆ†ä¼ æ„Ÿå™¨æ•…éšœ
- **æ¨è**ï¼šKNNæ’å€¼æˆ–æ—¶é—´åºåˆ—æ’å€¼
- **åŸå› **ï¼šä¼ æ„Ÿå™¨é—´é€šå¸¸æœ‰ç›¸å…³æ€§

#### åœºæ™¯2ï¼šé—®å·è°ƒæŸ¥æ•°æ®
- **ç‰¹ç‚¹**ï¼šéšæœºç¼ºå¤±ï¼Œæ ·æœ¬çè´µ
- **æ¨è**ï¼šè¿­ä»£æ’å€¼
- **åŸå› **ï¼šä¸èƒ½éšæ„åˆ é™¤æ ·æœ¬ï¼Œéœ€è¦ç²¾ç¡®å¡«å……

#### åœºæ™¯3ï¼šç½‘ç»œçˆ¬è™«æ•°æ®
- **ç‰¹ç‚¹**ï¼šç»“æ„åŒ–ç¼ºå¤±ï¼Œæ•°æ®é‡å¤§
- **æ¨è**ï¼šæ··åˆæ–¹æ³•
- **åŸå› **ï¼šå¯ä»¥æ‰¿å—ä¸€å®šæ•°æ®æŸå¤±ï¼Œè¿½æ±‚æ•ˆç‡

#### åœºæ™¯4ï¼šåŒ»ç–—æ•°æ®
- **ç‰¹ç‚¹**ï¼šç¼ºå¤±æœ‰æ„ä¹‰ï¼Œæ ·æœ¬å®è´µ
- **æ¨è**ï¼šé¢†åŸŸçŸ¥è¯†+KNNæ’å€¼
- **åŸå› **ï¼šéœ€è¦ç»“åˆåŒ»å­¦çŸ¥è¯†åˆ¤æ–­

## ğŸ’» å®é™…åº”ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæˆ¿ä»·é¢„æµ‹æ•°æ®

```python
import numpy as np
from nan_handling_guide import NaNHandler

# æ¨¡æ‹Ÿæˆ¿ä»·æ•°æ®ï¼ˆå«NaNï¼‰
np.random.seed(42)
n_samples = 1000

# åˆ›å»ºå«NaNçš„ç‰¹å¾æ•°æ®
area = np.random.normal(100, 30, n_samples)
rooms = np.random.randint(1, 6, n_samples).astype(float)
age = np.random.randint(0, 50, n_samples).astype(float)

# å¼•å…¥NaNï¼ˆæ¨¡æ‹Ÿæ•°æ®æ”¶é›†é—®é¢˜ï¼‰
area[np.random.choice(n_samples, 50)] = np.nan      # 5%ç¼ºå¤±
rooms[np.random.choice(n_samples, 100)] = np.nan    # 10%ç¼ºå¤±
age[np.random.choice(n_samples, 200)] = np.nan      # 20%ç¼ºå¤±

X = np.column_stack([area, rooms, age])
y = area * 0.5 + rooms * 10 + (50 - age) * 0.2 + np.random.normal(0, 5, n_samples)

print(f"åŸå§‹æ•°æ®: {X.shape}")
print(f"NaNæ¯”ä¾‹: {np.isnan(X).sum() / X.size * 100:.1f}%")

# å¤„ç†NaN
handler = NaNHandler()
analysis = handler.analyze_nan_pattern(X, y, ['é¢ç§¯', 'æˆ¿é—´æ•°', 'æˆ¿é¾„'])
handler.print_nan_summary(analysis)

# åº”ç”¨æ¨èç­–ç•¥
strategy = handler.recommend_strategy(analysis)
if strategy == 'knn':
    X_clean, y_clean = handler.strategy_4_knn_imputation(X, y)
elif strategy == 'hybrid':
    X_clean, y_clean = handler.strategy_6_hybrid_approach(X, y)

print(f"å¤„ç†åæ•°æ®: {X_clean.shape}")
print(f"æ•°æ®ä¿ç•™ç‡: {X_clean.shape[0] / X.shape[0] * 100:.1f}%")
```

### ç¤ºä¾‹2ï¼šæ—¶é—´åºåˆ—ä¼ æ„Ÿå™¨æ•°æ®

```python
# æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®
n_timesteps = 2000
n_sensors = 8

# ç”ŸæˆåŸºç¡€æ—¶é—´åºåˆ—
time_series = np.random.randn(n_timesteps, n_sensors)
for i in range(n_sensors):
    # æ·»åŠ è¶‹åŠ¿å’Œå­£èŠ‚æ€§
    trend = np.linspace(0, 2, n_timesteps)
    seasonal = np.sin(2 * np.pi * np.arange(n_timesteps) / 100)
    time_series[:, i] += trend + seasonal

# æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•…éšœï¼ˆè¿ç»­ç¼ºå¤±ï¼‰
for sensor in range(n_sensors):
    if np.random.random() < 0.3:  # 30%æ¦‚ç‡æ•…éšœ
        fault_start = np.random.randint(0, n_timesteps - 100)
        fault_duration = np.random.randint(20, 100)
        time_series[fault_start:fault_start+fault_duration, sensor] = np.nan

# è½¬æ¢ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜
def create_sequences(data, lookback=24, forecast=6):
    X, y = [], []
    for i in range(lookback, len(data) - forecast):
        X.append(data[i-lookback:i].flatten())
        y.append(data[i:i+forecast, 0])  # é¢„æµ‹ç¬¬ä¸€ä¸ªä¼ æ„Ÿå™¨
    return np.array(X), np.array(y)

X, y = create_sequences(time_series)

print(f"æ—¶é—´åºåˆ—æ•°æ®: {X.shape}")
print(f"NaNæ¯”ä¾‹: {np.isnan(X).sum() / X.size * 100:.1f}%")

# å¤„ç†NaN
handler = NaNHandler()
X_clean, y_clean = handler.strategy_4_knn_imputation(X, y, n_neighbors=10)

print(f"å¤„ç†å: {X_clean.shape}")
```

### ç¤ºä¾‹3ï¼šå®Œæ•´è®­ç»ƒæµç¨‹

```python
def train_with_nan_handling(X_with_nan, y_with_nan):
    """å«NaNæ•°æ®çš„å®Œæ•´è®­ç»ƒæµç¨‹"""
    
    print("=== NaNæ•°æ®å¤„ç†ä¸æ¨¡å‹è®­ç»ƒ ===")
    
    # 1. NaNåˆ†æä¸å¤„ç†
    from nan_handling_guide import NaNHandler
    handler = NaNHandler()
    
    print("1. åˆ†æNaNæ¨¡å¼...")
    analysis = handler.analyze_nan_pattern(X_with_nan, y_with_nan)
    handler.print_nan_summary(analysis)
    
    print("2. é€‰æ‹©å¤„ç†ç­–ç•¥...")
    strategy = handler.recommend_strategy(analysis)
    
    print("3. åº”ç”¨å¤„ç†ç­–ç•¥...")
    if strategy == 'remove_samples':
        X_clean, y_clean = handler.strategy_1_remove_samples(X_with_nan, y_with_nan)
    elif strategy == 'knn':
        X_clean, y_clean = handler.strategy_4_knn_imputation(X_with_nan, y_with_nan)
    elif strategy == 'hybrid':
        X_clean, y_clean = handler.strategy_6_hybrid_approach(X_with_nan, y_with_nan)
    else:
        X_clean, y_clean = handler.strategy_3_simple_imputation(X_with_nan, y_with_nan)
    
    print(f"å¤„ç†å®Œæˆ: {X_with_nan.shape} -> {X_clean.shape}")
    
    # 2. éªŒè¯æ•°æ®è´¨é‡
    assert not np.isnan(X_clean).any(), "è¾“å…¥ç‰¹å¾ä»æœ‰NaN"
    assert not np.isnan(y_clean).any(), "ç›®æ ‡å€¼ä»æœ‰NaN"
    print("âœ… æ•°æ®è´¨é‡éªŒè¯é€šè¿‡")
    
    # 3. æ¨¡å‹è®­ç»ƒ
    import yaml
    from data_processor import DataProcessor
    from mlp_model import create_model_from_config
    from trainer import MLPTrainer
    from evaluator import ModelEvaluator
    
    print("4. å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    config = yaml.safe_load(open('config.yaml', 'r', encoding='utf-8'))
    
    # æ•°æ®å¤„ç†
    processor = DataProcessor(config)
    processor.load_data_from_arrays(X_clean, y_clean)
    processor.normalize_data()
    
    # æ•°æ®åˆ†å‰²
    X_train, X_val, X_test, y_train, y_val, y_test = processor.split_data()
    train_loader, val_loader, test_loader = processor.create_data_loaders(
        X_train, X_val, X_test, y_train, y_val, y_test
    )
    
    # æ¨¡å‹åˆ›å»ºä¸è®­ç»ƒ
    model = create_model_from_config(config, processor.input_dim, processor.output_dim)
    trainer = MLPTrainer(model, config)
    
    history = trainer.train(train_loader, val_loader)
    
    # 4. æ¨¡å‹è¯„ä¼°
    print("5. æ¨¡å‹è¯„ä¼°...")
    evaluator = ModelEvaluator(save_plots=False)
    
    test_pred, test_true = trainer.predict(test_loader)
    test_pred_orig = processor.inverse_transform_predictions(test_pred)
    test_true_orig = processor.inverse_transform_predictions(test_true)
    
    metrics = evaluator.evaluate_model(test_true_orig, test_pred_orig)
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æœ€ç»ˆRÂ²åˆ†æ•°: {metrics['r2']:.4f}")
    print(f"   RMSE: {metrics['rmse']:.4f}")
    print(f"   æ•°æ®åˆ©ç”¨ç‡: {X_clean.shape[0] / X_with_nan.shape[0] * 100:.1f}%")
    
    return trainer, processor, metrics, handler

# ä½¿ç”¨ç¤ºä¾‹
# trainer, processor, metrics, handler = train_with_nan_handling(your_X, your_y)
```

## ğŸ¯ æœ€ä½³å®è·µå»ºè®®

### 1. é¢„å¤„ç†é˜¶æ®µ

#### æ•°æ®æ¢ç´¢
```python
# å…¨é¢äº†è§£NaNåˆ†å¸ƒ
def explore_nan_pattern(X, y, feature_names=None):
    """æ¢ç´¢NaNåˆ†å¸ƒæ¨¡å¼"""
    
    print("=== NaNåˆ†å¸ƒæ¢ç´¢ ===")
    
    # åŸºç¡€ç»Ÿè®¡
    total_cells = X.size + y.size
    nan_cells = np.isnan(X).sum() + np.isnan(y).sum()
    print(f"æ€»ä½“NaNæ¯”ä¾‹: {nan_cells / total_cells * 100:.2f}%")
    
    # ç‰¹å¾çº§åˆ«åˆ†æ
    if feature_names is None:
        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
    
    feature_nan_counts = np.isnan(X).sum(axis=0)
    feature_nan_ratios = feature_nan_counts / X.shape[0]
    
    print("\nç‰¹å¾ç¼ºå¤±æƒ…å†µ:")
    for name, count, ratio in zip(feature_names, feature_nan_counts, feature_nan_ratios):
        if count > 0:
            print(f"  {name}: {count} ({ratio*100:.1f}%)")
    
    # æ ·æœ¬çº§åˆ«åˆ†æ
    sample_nan_counts = np.isnan(X).sum(axis=1)
    complete_samples = np.sum(sample_nan_counts == 0)
    print(f"\nå®Œæ•´æ ·æœ¬: {complete_samples} / {X.shape[0]} ({complete_samples/X.shape[0]*100:.1f}%)")
    
    # ç¼ºå¤±æ¨¡å¼åˆ†æ
    unique_patterns = np.unique(np.isnan(X), axis=0)
    print(f"å”¯ä¸€ç¼ºå¤±æ¨¡å¼æ•°: {len(unique_patterns)}")
    
    return {
        'feature_nan_ratios': dict(zip(feature_names, feature_nan_ratios)),
        'complete_samples_ratio': complete_samples / X.shape[0],
        'total_nan_ratio': nan_cells / total_cells
    }
```

#### è´¨é‡è¯„ä¼°
```python
def assess_data_quality(X, y):
    """è¯„ä¼°æ•°æ®è´¨é‡"""
    
    quality_score = 100  # æ»¡åˆ†100
    issues = []
    
    # NaNæ¯”ä¾‹æ‰£åˆ†
    nan_ratio = (np.isnan(X).sum() + np.isnan(y).sum()) / (X.size + y.size)
    if nan_ratio > 0.5:
        quality_score -= 50
        issues.append("ä¸¥é‡ç¼ºå¤±ï¼ˆ>50%ï¼‰")
    elif nan_ratio > 0.2:
        quality_score -= 20
        issues.append("ä¸­åº¦ç¼ºå¤±ï¼ˆ20-50%ï¼‰")
    elif nan_ratio > 0.05:
        quality_score -= 5
        issues.append("è½»åº¦ç¼ºå¤±ï¼ˆ5-20%ï¼‰")
    
    # å®Œæ•´æ ·æœ¬æ¯”ä¾‹
    complete_ratio = np.sum(~np.isnan(X).any(axis=1)) / X.shape[0]
    if complete_ratio < 0.3:
        quality_score -= 30
        issues.append("å®Œæ•´æ ·æœ¬è¿‡å°‘ï¼ˆ<30%ï¼‰")
    elif complete_ratio < 0.7:
        quality_score -= 10
        issues.append("å®Œæ•´æ ·æœ¬è¾ƒå°‘ï¼ˆ30-70%ï¼‰")
    
    # é«˜ç¼ºå¤±ç‰¹å¾
    high_missing_features = np.sum(np.isnan(X).mean(axis=0) > 0.7)
    if high_missing_features > X.shape[1] * 0.3:
        quality_score -= 20
        issues.append(f"é«˜ç¼ºå¤±ç‰¹å¾è¿‡å¤šï¼ˆ{high_missing_features}ä¸ªï¼‰")
    
    print(f"æ•°æ®è´¨é‡è¯„åˆ†: {quality_score}/100")
    if issues:
        print("ä¸»è¦é—®é¢˜:")
        for issue in issues:
            print(f"  - {issue}")
    
    return quality_score, issues
```

### 2. å¤„ç†ç­–ç•¥é€‰æ‹©

#### ä¸šåŠ¡å¯¼å‘é€‰æ‹©
```python
def business_oriented_strategy(X, y, business_context):
    """åŸºäºä¸šåŠ¡åœºæ™¯é€‰æ‹©ç­–ç•¥"""
    
    if business_context == 'medical':
        # åŒ»ç–—æ•°æ®ï¼šæ ·æœ¬çè´µï¼Œéœ€è¦ç²¾ç¡®å¡«å……
        return 'iterative'
    elif business_context == 'financial':
        # é‡‘èæ•°æ®ï¼šå¯¹å‡†ç¡®æ€§è¦æ±‚é«˜ï¼Œä¿å®ˆå¤„ç†
        return 'remove_samples'
    elif business_context == 'iot':
        # ç‰©è”ç½‘æ•°æ®ï¼šæ•°æ®é‡å¤§ï¼Œå¯ä»¥å®¹å¿ä¸€å®šæŸå¤±
        return 'hybrid'
    elif business_context == 'survey':
        # è°ƒç ”æ•°æ®ï¼šæ ·æœ¬è·å–æˆæœ¬é«˜
        return 'knn'
    else:
        # é€šç”¨åœºæ™¯ï¼šå¹³è¡¡æ•ˆæœ
        return 'hybrid'
```

#### æ€§èƒ½å¯¼å‘é€‰æ‹©
```python
def performance_oriented_strategy(X, y, time_constraint='medium'):
    """åŸºäºæ€§èƒ½è¦æ±‚é€‰æ‹©ç­–ç•¥"""
    
    data_size = X.shape[0] * X.shape[1]
    
    if time_constraint == 'fast':
        if data_size > 1000000:  # å¤§æ•°æ®
            return 'remove_samples'
        else:
            return 'simple_mean'
    elif time_constraint == 'medium':
        return 'knn'
    else:  # 'slow' - è¿½æ±‚æœ€ä½³æ•ˆæœ
        return 'iterative'
```

### 3. å¤„ç†åéªŒè¯

#### æ•°æ®åˆ†å¸ƒæ£€æŸ¥
```python
def validate_imputation_quality(X_original, X_imputed, feature_names=None):
    """éªŒè¯æ’å€¼è´¨é‡"""
    
    if feature_names is None:
        feature_names = [f'Feature_{i}' for i in range(X_original.shape[1])]
    
    print("=== æ’å€¼è´¨é‡éªŒè¯ ===")
    
    for i, name in enumerate(feature_names):
        original_col = X_original[:, i]
        imputed_col = X_imputed[:, i]
        
        # åªæ¯”è¾ƒåŸæœ¬ä¸æ˜¯NaNçš„å€¼
        mask = ~np.isnan(original_col)
        if mask.sum() == 0:
            continue
            
        original_clean = original_col[mask]
        imputed_clean = imputed_col[mask]
        
        # ç»Ÿè®¡é‡æ¯”è¾ƒ
        orig_mean, orig_std = np.mean(original_clean), np.std(original_clean)
        imp_mean, imp_std = np.mean(imputed_clean), np.std(imputed_clean)
        
        mean_diff = abs(orig_mean - imp_mean) / orig_mean * 100
        std_diff = abs(orig_std - imp_std) / orig_std * 100
        
        print(f"{name}:")
        print(f"  å‡å€¼å˜åŒ–: {mean_diff:.1f}%")
        print(f"  æ ‡å‡†å·®å˜åŒ–: {std_diff:.1f}%")
        
        if mean_diff > 10 or std_diff > 20:
            print(f"  âš ï¸ åˆ†å¸ƒå˜åŒ–è¾ƒå¤§")
        else:
            print(f"  âœ… åˆ†å¸ƒä¿æŒè‰¯å¥½")
```

#### æ¨¡å‹æ€§èƒ½å¯¹æ¯”
```python
def compare_model_performance(X_original, y, strategies=['remove_samples', 'knn', 'hybrid']):
    """æ¯”è¾ƒä¸åŒç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“"""
    
    from sklearn.model_selection import cross_val_score
    from sklearn.ensemble import RandomForestRegressor
    from nan_handling_guide import NaNHandler
    
    handler = NaNHandler()
    results = {}
    
    for strategy in strategies:
        try:
            # åº”ç”¨ç­–ç•¥
            if strategy == 'remove_samples':
                X_clean, y_clean = handler.strategy_1_remove_samples(X_original, y)
            elif strategy == 'knn':
                X_clean, y_clean = handler.strategy_4_knn_imputation(X_original, y)
            elif strategy == 'hybrid':
                X_clean, y_clean = handler.strategy_6_hybrid_approach(X_original, y)
            
            # äº¤å‰éªŒè¯è¯„ä¼°
            model = RandomForestRegressor(n_estimators=50, random_state=42)
            scores = cross_val_score(model, X_clean, y_clean, cv=5, scoring='r2')
            
            results[strategy] = {
                'mean_r2': scores.mean(),
                'std_r2': scores.std(),
                'data_retention': X_clean.shape[0] / X_original.shape[0],
                'feature_retention': X_clean.shape[1] / X_original.shape[1]
            }
            
        except Exception as e:
            results[strategy] = {'error': str(e)}
    
    # æ‰“å°æ¯”è¾ƒç»“æœ
    print("=== ç­–ç•¥æ€§èƒ½æ¯”è¾ƒ ===")
    for strategy, result in results.items():
        if 'error' in result:
            print(f"{strategy}: å¤±è´¥ - {result['error']}")
        else:
            print(f"{strategy}:")
            print(f"  RÂ²åˆ†æ•°: {result['mean_r2']:.4f} Â± {result['std_r2']:.4f}")
            print(f"  æ•°æ®ä¿ç•™: {result['data_retention']*100:.1f}%")
            print(f"  ç‰¹å¾ä¿ç•™: {result['feature_retention']*100:.1f}%")
    
    return results
```

### 4. ç”Ÿäº§ç¯å¢ƒè€ƒè™‘

#### ä¸€è‡´æ€§å¤„ç†
```python
class ProductionNaNHandler:
    """ç”Ÿäº§ç¯å¢ƒNaNå¤„ç†å™¨"""
    
    def __init__(self):
        self.imputers = {}
        self.strategy = None
        self.fitted = False
    
    def fit(self, X_train, y_train, strategy='auto'):
        """åœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆå¤„ç†å™¨"""
        
        if strategy == 'auto':
            handler = NaNHandler()
            analysis = handler.analyze_nan_pattern(X_train, y_train)
            self.strategy = handler.recommend_strategy(analysis)
        else:
            self.strategy = strategy
        
        # æ ¹æ®ç­–ç•¥æ‹Ÿåˆç›¸åº”çš„å¤„ç†å™¨
        if self.strategy == 'knn':
            from sklearn.impute import KNNImputer
            self.imputers['X'] = KNNImputer(n_neighbors=5)
            self.imputers['X'].fit(X_train)
            
            if np.isnan(y_train).any():
                self.imputers['y'] = KNNImputer(n_neighbors=5)
                self.imputers['y'].fit(y_train.reshape(-1, 1))
        
        elif self.strategy == 'simple_mean':
            from sklearn.impute import SimpleImputer
            self.imputers['X'] = SimpleImputer(strategy='mean')
            self.imputers['X'].fit(X_train)
            
            if np.isnan(y_train).any():
                self.imputers['y'] = SimpleImputer(strategy='mean')
                self.imputers['y'].fit(y_train.reshape(-1, 1))
        
        self.fitted = True
        print(f"ç”Ÿäº§ç¯å¢ƒå¤„ç†å™¨å·²æ‹Ÿåˆï¼Œç­–ç•¥: {self.strategy}")
    
    def transform(self, X, y=None):
        """è½¬æ¢æ–°æ•°æ®"""
        
        if not self.fitted:
            raise ValueError("å¤„ç†å™¨æœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit()")
        
        if self.strategy in ['knn', 'simple_mean']:
            X_clean = self.imputers['X'].transform(X)
            
            if y is not None and 'y' in self.imputers:
                y_clean = self.imputers['y'].transform(y.reshape(-1, 1)).ravel()
                return X_clean, y_clean
            else:
                return X_clean
        
        else:
            # å¯¹äºåˆ é™¤ç­–ç•¥ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦ç‰¹æ®Šå¤„ç†
            print("è­¦å‘Š: åˆ é™¤ç­–ç•¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯èƒ½å¯¼è‡´æ•°æ®ä¸ä¸€è‡´")
            return X, y
    
    def save(self, filepath):
        """ä¿å­˜å¤„ç†å™¨"""
        import joblib
        joblib.dump({
            'imputers': self.imputers,
            'strategy': self.strategy,
            'fitted': self.fitted
        }, filepath)
    
    def load(self, filepath):
        """åŠ è½½å¤„ç†å™¨"""
        import joblib
        data = joblib.load(filepath)
        self.imputers = data['imputers']
        self.strategy = data['strategy']
        self.fitted = data['fitted']

# ä½¿ç”¨ç¤ºä¾‹
# è®­ç»ƒé˜¶æ®µ
handler = ProductionNaNHandler()
handler.fit(X_train, y_train)
handler.save('nan_handler.pkl')

# ç”Ÿäº§é˜¶æ®µ
handler = ProductionNaNHandler()
handler.load('nan_handler.pkl')
X_new_clean = handler.transform(X_new)
```

## â“ å¸¸è§é—®é¢˜è§£ç­”

### Q1: å¦‚ä½•åˆ¤æ–­NaNæ˜¯éšæœºç¼ºå¤±è¿˜æ˜¯ç³»ç»Ÿæ€§ç¼ºå¤±ï¼Ÿ

**A1**: å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•åˆ¤æ–­ï¼š

```python
def analyze_missing_pattern(X, feature_names=None):
    """åˆ†æç¼ºå¤±æ¨¡å¼"""
    
    # è®¡ç®—ç‰¹å¾é—´ç¼ºå¤±çš„ç›¸å…³æ€§
    nan_matrix = np.isnan(X).astype(int)
    correlation_matrix = np.corrcoef(nan_matrix.T)
    
    # é«˜ç›¸å…³æ€§è¡¨æ˜ç³»ç»Ÿæ€§ç¼ºå¤±
    high_corr_pairs = []
    for i in range(len(correlation_matrix)):
        for j in range(i+1, len(correlation_matrix)):
            if abs(correlation_matrix[i, j]) > 0.5:
                high_corr_pairs.append((i, j, correlation_matrix[i, j]))
    
    if high_corr_pairs:
        print("å‘ç°ç³»ç»Ÿæ€§ç¼ºå¤±æ¨¡å¼:")
        for i, j, corr in high_corr_pairs:
            name_i = feature_names[i] if feature_names else f"Feature_{i}"
            name_j = feature_names[j] if feature_names else f"Feature_{j}"
            print(f"  {name_i} - {name_j}: ç›¸å…³æ€§ {corr:.3f}")
    else:
        print("ç¼ºå¤±æ¨¡å¼ç›¸å¯¹éšæœº")
    
    return high_corr_pairs
```

### Q2: æ’å€¼åå¦‚ä½•éªŒè¯ç»“æœçš„åˆç†æ€§ï¼Ÿ

**A2**: å¤šè§’åº¦éªŒè¯ï¼š

```python
def validate_imputation_results(X_original, X_imputed):
    """éªŒè¯æ’å€¼ç»“æœ"""
    
    # 1. ç»Ÿè®¡é‡æ£€æŸ¥
    for i in range(X_original.shape[1]):
        original_col = X_original[:, i]
        imputed_col = X_imputed[:, i]
        
        # åŸå§‹æ•°æ®çš„ç»Ÿè®¡é‡ï¼ˆæ’é™¤NaNï¼‰
        orig_clean = original_col[~np.isnan(original_col)]
        orig_mean, orig_std = np.mean(orig_clean), np.std(orig_clean)
        
        # æ’å€¼æ•°æ®çš„ç»Ÿè®¡é‡
        imp_mean, imp_std = np.mean(imputed_col), np.std(imputed_col)
        
        print(f"ç‰¹å¾ {i}:")
        print(f"  åŸå§‹: å‡å€¼={orig_mean:.3f}, æ ‡å‡†å·®={orig_std:.3f}")
        print(f"  æ’å€¼: å‡å€¼={imp_mean:.3f}, æ ‡å‡†å·®={imp_std:.3f}")
    
    # 2. åˆ†å¸ƒæ£€æŸ¥
    from scipy import stats
    for i in range(X_original.shape[1]):
        original_col = X_original[:, i]
        imputed_col = X_imputed[:, i]
        
        orig_clean = original_col[~np.isnan(original_col)]
        
        # KSæ£€éªŒæ¯”è¾ƒåˆ†å¸ƒ
        ks_stat, p_value = stats.ks_2samp(orig_clean, imputed_col)
        
        if p_value < 0.05:
            print(f"âš ï¸ ç‰¹å¾ {i} åˆ†å¸ƒå‘ç”Ÿæ˜¾è‘—å˜åŒ– (p={p_value:.4f})")
        else:
            print(f"âœ… ç‰¹å¾ {i} åˆ†å¸ƒä¿æŒä¸€è‡´ (p={p_value:.4f})")
    
    # 3. å¼‚å¸¸å€¼æ£€æŸ¥
    for i in range(X_original.shape[1]):
        imputed_col = X_imputed[:, i]
        
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        Q1, Q3 = np.percentile(imputed_col, [25, 75])
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = np.sum((imputed_col < lower_bound) | (imputed_col > upper_bound))
        outlier_ratio = outliers / len(imputed_col)
        
        if outlier_ratio > 0.1:
            print(f"âš ï¸ ç‰¹å¾ {i} å¼‚å¸¸å€¼æ¯”ä¾‹è¿‡é«˜: {outlier_ratio*100:.1f}%")
```

### Q3: å¤§æ•°æ®é›†å¦‚ä½•é«˜æ•ˆå¤„ç†NaNï¼Ÿ

**A3**: åˆ†å—å¤„ç†å’Œå¹¶è¡ŒåŒ–ï¼š

```python
def handle_large_dataset_nan(X, y, chunk_size=10000, n_jobs=4):
    """å¤§æ•°æ®é›†NaNå¤„ç†"""
    
    from joblib import Parallel, delayed
    import numpy as np
    
    def process_chunk(X_chunk, y_chunk):
        """å¤„ç†å•ä¸ªæ•°æ®å—"""
        handler = NaNHandler()
        return handler.strategy_4_knn_imputation(X_chunk, y_chunk)
    
    # åˆ†å—å¤„ç†
    n_samples = X.shape[0]
    chunks = [(i, min(i + chunk_size, n_samples)) for i in range(0, n_samples, chunk_size)]
    
    print(f"åˆ†ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œå¹¶è¡Œå¤„ç†...")
    
    # å¹¶è¡Œå¤„ç†å„å—
    results = Parallel(n_jobs=n_jobs)(
        delayed(process_chunk)(X[start:end], y[start:end]) 
        for start, end in chunks
    )
    
    # åˆå¹¶ç»“æœ
    X_clean_chunks, y_clean_chunks = zip(*results)
    X_clean = np.vstack(X_clean_chunks)
    y_clean = np.hstack(y_clean_chunks)
    
    return X_clean, y_clean
```

### Q4: å¦‚ä½•å¤„ç†æ—¶é—´åºåˆ—ä¸­çš„NaNï¼Ÿ

**A4**: æ—¶é—´åºåˆ—ä¸“ç”¨æ–¹æ³•ï¼š

```python
def handle_timeseries_nan(ts_data, method='interpolate'):
    """æ—¶é—´åºåˆ—NaNå¤„ç†"""
    
    import pandas as pd
    
    # è½¬æ¢ä¸ºpandasæ—¶é—´åºåˆ—
    ts = pd.Series(ts_data)
    
    if method == 'interpolate':
        # çº¿æ€§æ’å€¼
        ts_clean = ts.interpolate(method='linear')
    elif method == 'forward_fill':
        # å‰å‘å¡«å……
        ts_clean = ts.fillna(method='ffill')
    elif method == 'backward_fill':
        # åå‘å¡«å……
        ts_clean = ts.fillna(method='bfill')
    elif method == 'seasonal':
        # å­£èŠ‚æ€§æ’å€¼
        ts_clean = ts.interpolate(method='seasonal', period=24)  # å‡è®¾24å°æ—¶å‘¨æœŸ
    
    return ts_clean.values
```

### Q5: æ’å€¼ä¼šä¸ä¼šå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Ÿ

**A5**: åˆç†çš„æ’å€¼é€šå¸¸ä¸ä¼šæ˜¾è‘—å½±å“æ³›åŒ–èƒ½åŠ›ï¼Œä½†éœ€è¦æ³¨æ„ï¼š

1. **é¿å…è¿‡åº¦æ‹Ÿåˆæ’å€¼**ï¼šä¸è¦ä½¿ç”¨è¿‡äºå¤æ‚çš„æ’å€¼æ–¹æ³•
2. **ä¿æŒéªŒè¯é›†ç‹¬ç«‹**ï¼šæ’å€¼å™¨åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ
3. **ç›‘æ§æ€§èƒ½å˜åŒ–**ï¼šæ¯”è¾ƒæ’å€¼å‰åçš„äº¤å‰éªŒè¯ç»“æœ
4. **è€ƒè™‘ä¸ç¡®å®šæ€§**ï¼šå¯ä»¥ä½¿ç”¨å¤šé‡æ’å€¼æ¥é‡åŒ–ä¸ç¡®å®šæ€§

```python
def multiple_imputation_uncertainty(X, y, n_imputations=5):
    """å¤šé‡æ’å€¼è¯„ä¼°ä¸ç¡®å®šæ€§"""
    
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import cross_val_score
    
    scores = []
    
    for i in range(n_imputations):
        # æ¯æ¬¡ä½¿ç”¨ä¸åŒéšæœºç§å­è¿›è¡Œæ’å€¼
        handler = NaNHandler()
        X_imp, y_imp = handler.strategy_4_knn_imputation(X, y)
        
        # è¯„ä¼°æ¨¡å‹æ€§èƒ½
        model = RandomForestRegressor(random_state=i)
        cv_scores = cross_val_score(model, X_imp, y_imp, cv=5)
        scores.append(cv_scores.mean())
    
    mean_score = np.mean(scores)
    std_score = np.std(scores)
    
    print(f"å¤šé‡æ’å€¼ç»“æœ: {mean_score:.4f} Â± {std_score:.4f}")
    print(f"ä¸ç¡®å®šæ€§: {std_score/mean_score*100:.1f}%")
    
    return mean_score, std_score
```

## ğŸ“ æ€»ç»“

NaNå€¼å¤„ç†æ˜¯æ•°æ®é¢„å¤„ç†ä¸­çš„å…³é”®æ­¥éª¤ï¼Œé€‰æ‹©åˆé€‚çš„ç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚æœ¬æŒ‡å—æä¾›çš„å·¥å…·å’Œå»ºè®®å¯ä»¥å¸®åŠ©æ‚¨ï¼š

1. **ç³»ç»Ÿåˆ†æ**ï¼šå…¨é¢äº†è§£æ•°æ®ä¸­çš„ç¼ºå¤±æ¨¡å¼
2. **æ™ºèƒ½é€‰æ‹©**ï¼šæ ¹æ®æ•°æ®ç‰¹ç‚¹è‡ªåŠ¨æ¨èæœ€ä½³ç­–ç•¥
3. **è´¨é‡éªŒè¯**ï¼šç¡®ä¿å¤„ç†åçš„æ•°æ®è´¨é‡
4. **ç”Ÿäº§éƒ¨ç½²**ï¼šä¿æŒè®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„ä¸€è‡´æ€§

**æ ¸å¿ƒå»ºè®®**ï¼š
- å…ˆåˆ†æå†å¤„ç†ï¼Œäº†è§£ç¼ºå¤±çš„åŸå› å’Œæ¨¡å¼
- ä¼˜å…ˆè€ƒè™‘æ··åˆæ–¹æ³•ï¼Œå¹³è¡¡æ•°æ®ä¿ç•™å’Œè´¨é‡
- éªŒè¯å¤„ç†ç»“æœï¼Œç¡®ä¿åˆ†å¸ƒå’Œç»Ÿè®¡ç‰¹æ€§åˆç†
- åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¿æŒå¤„ç†æ–¹å¼çš„ä¸€è‡´æ€§

é€šè¿‡åˆç†çš„NaNå¤„ç†ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„è®­ç»ƒæ•ˆæœå’Œæ³›åŒ–èƒ½åŠ›ï¼