"""
NaNå€¼å¤„ç†æŒ‡å—å’Œå·¥å…·
æä¾›å¤šç§å¤„ç†ç¼ºå¤±å€¼çš„ç­–ç•¥å’Œå®ç°æ–¹æ³•
"""

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Tuple, Dict, Any, Optional, List
from loguru import logger


class NaNHandler:
    """NaNå€¼å¤„ç†å™¨ï¼Œæä¾›å¤šç§ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥"""
    
    def __init__(self):
        self.imputers = {}
        self.nan_info = {}
        
    def analyze_nan_pattern(self, X: np.ndarray, y: np.ndarray, 
                           feature_names: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        åˆ†æNaNå€¼çš„åˆ†å¸ƒæ¨¡å¼
        
        Args:
            X: è¾“å…¥ç‰¹å¾æ•°ç»„
            y: ç›®æ ‡å€¼æ•°ç»„
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            
        Returns:
            NaNåˆ†æç»“æœå­—å…¸
        """
        if feature_names is None:
            feature_names = [f'feature_{i}' for i in range(X.shape[1])]
            
        # ç»Ÿè®¡æ¯ä¸ªç‰¹å¾çš„NaNæ•°é‡
        nan_counts_X = np.isnan(X).sum(axis=0)
        nan_ratios_X = nan_counts_X / X.shape[0]
        
        # ç»Ÿè®¡ç›®æ ‡å€¼çš„NaNæ•°é‡
        nan_counts_y = np.isnan(y).sum(axis=0) if y.ndim > 1 else np.isnan(y).sum()
        nan_ratios_y = nan_counts_y / y.shape[0]
        
        # ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„NaNæ•°é‡
        nan_per_sample = np.isnan(X).sum(axis=1)
        
        # å®Œå…¨ç¼ºå¤±çš„æ ·æœ¬æ•°é‡
        complete_missing_samples = np.sum(nan_per_sample == X.shape[1])
        
        # å®Œå…¨å®Œæ•´çš„æ ·æœ¬æ•°é‡
        complete_samples = np.sum(nan_per_sample == 0)
        
        analysis = {
            'total_samples': X.shape[0],
            'total_features': X.shape[1],
            'feature_names': feature_names,
            'nan_counts_per_feature': dict(zip(feature_names, nan_counts_X)),
            'nan_ratios_per_feature': dict(zip(feature_names, nan_ratios_X)),
            'nan_counts_targets': nan_counts_y,
            'nan_ratios_targets': nan_ratios_y,
            'samples_with_nan': np.sum(nan_per_sample > 0),
            'complete_samples': complete_samples,
            'complete_missing_samples': complete_missing_samples,
            'max_nan_per_sample': np.max(nan_per_sample),
            'avg_nan_per_sample': np.mean(nan_per_sample),
            'features_with_high_nan': [name for name, ratio in zip(feature_names, nan_ratios_X) if ratio > 0.5]
        }
        
        self.nan_info = analysis
        return analysis
    
    def print_nan_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°NaNåˆ†ææ‘˜è¦"""
        print("=" * 60)
        print("ğŸ” NaNå€¼åˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        print(f"ğŸ“Š æ•°æ®æ¦‚å†µ:")
        print(f"  æ€»æ ·æœ¬æ•°: {analysis['total_samples']}")
        print(f"  æ€»ç‰¹å¾æ•°: {analysis['total_features']}")
        print(f"  å®Œæ•´æ ·æœ¬æ•°: {analysis['complete_samples']} ({analysis['complete_samples']/analysis['total_samples']*100:.1f}%)")
        print(f"  å«ç¼ºå¤±å€¼æ ·æœ¬æ•°: {analysis['samples_with_nan']} ({analysis['samples_with_nan']/analysis['total_samples']*100:.1f}%)")
        
        print(f"\nğŸ¯ ç›®æ ‡å€¼ç¼ºå¤±æƒ…å†µ:")
        if isinstance(analysis['nan_counts_targets'], np.ndarray):
            for i, (count, ratio) in enumerate(zip(analysis['nan_counts_targets'], analysis['nan_ratios_targets'])):
                print(f"  ç›®æ ‡{i+1}: {count}ä¸ªç¼ºå¤± ({ratio*100:.1f}%)")
        else:
            print(f"  ç¼ºå¤±æ•°é‡: {analysis['nan_counts_targets']} ({analysis['nan_ratios_targets']*100:.1f}%)")
        
        print(f"\nğŸ“ˆ ç‰¹å¾ç¼ºå¤±æƒ…å†µ (å‰10ä¸ªæœ€ä¸¥é‡çš„):")
        sorted_features = sorted(analysis['nan_ratios_per_feature'].items(), 
                               key=lambda x: x[1], reverse=True)[:10]
        for name, ratio in sorted_features:
            if ratio > 0:
                count = analysis['nan_counts_per_feature'][name]
                print(f"  {name}: {count}ä¸ªç¼ºå¤± ({ratio*100:.1f}%)")
        
        if analysis['features_with_high_nan']:
            print(f"\nâš ï¸  é«˜ç¼ºå¤±ç‡ç‰¹å¾ (>50%): {len(analysis['features_with_high_nan'])}ä¸ª")
            for name in analysis['features_with_high_nan'][:5]:
                ratio = analysis['nan_ratios_per_feature'][name]
                print(f"    {name}: {ratio*100:.1f}%")
    
    def visualize_nan_pattern(self, X: np.ndarray, feature_names: Optional[List[str]] = None,
                             save_path: Optional[str] = None):
        """å¯è§†åŒ–NaNå€¼åˆ†å¸ƒæ¨¡å¼"""
        if feature_names is None:
            feature_names = [f'F{i}' for i in range(X.shape[1])]
            
        # åˆ›å»ºNaNæ©ç 
        nan_mask = np.isnan(X)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. æ¯ä¸ªç‰¹å¾çš„ç¼ºå¤±ç‡
        nan_ratios = nan_mask.mean(axis=0)
        axes[0, 0].bar(range(len(feature_names)), nan_ratios)
        axes[0, 0].set_title('æ¯ä¸ªç‰¹å¾çš„ç¼ºå¤±ç‡')
        axes[0, 0].set_xlabel('ç‰¹å¾ç´¢å¼•')
        axes[0, 0].set_ylabel('ç¼ºå¤±ç‡')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. æ¯ä¸ªæ ·æœ¬çš„ç¼ºå¤±æ•°é‡åˆ†å¸ƒ
        nan_per_sample = nan_mask.sum(axis=1)
        axes[0, 1].hist(nan_per_sample, bins=min(50, X.shape[1]), alpha=0.7)
        axes[0, 1].set_title('æ¯ä¸ªæ ·æœ¬çš„ç¼ºå¤±ç‰¹å¾æ•°é‡åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('ç¼ºå¤±ç‰¹å¾æ•°é‡')
        axes[0, 1].set_ylabel('æ ·æœ¬æ•°é‡')
        
        # 3. NaNæ¨¡å¼çƒ­åŠ›å›¾ (å–å‰100ä¸ªæ ·æœ¬å’Œå‰20ä¸ªç‰¹å¾)
        sample_size = min(100, X.shape[0])
        feature_size = min(20, X.shape[1])
        
        axes[1, 0].imshow(nan_mask[:sample_size, :feature_size], 
                         cmap='RdYlBu_r', aspect='auto')
        axes[1, 0].set_title(f'NaNæ¨¡å¼çƒ­åŠ›å›¾ (å‰{sample_size}æ ·æœ¬, å‰{feature_size}ç‰¹å¾)')
        axes[1, 0].set_xlabel('ç‰¹å¾ç´¢å¼•')
        axes[1, 0].set_ylabel('æ ·æœ¬ç´¢å¼•')
        
        # 4. ç¼ºå¤±ç‡åˆ†å¸ƒ
        axes[1, 1].hist(nan_ratios, bins=20, alpha=0.7)
        axes[1, 1].set_title('ç‰¹å¾ç¼ºå¤±ç‡åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('ç¼ºå¤±ç‡')
        axes[1, 1].set_ylabel('ç‰¹å¾æ•°é‡')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"NaNåˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
    
    def strategy_1_remove_samples(self, X: np.ndarray, y: np.ndarray, 
                                 threshold: float = 0.5) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç­–ç•¥1: åˆ é™¤å«æœ‰è¿‡å¤šNaNçš„æ ·æœ¬
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            y: ç›®æ ‡å€¼
            threshold: ç¼ºå¤±ç‡é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤æ¯”ä¾‹çš„æ ·æœ¬å°†è¢«åˆ é™¤
            
        Returns:
            æ¸…ç†åçš„X, y
        """
        nan_ratio_per_sample = np.isnan(X).sum(axis=1) / X.shape[1]
        mask = nan_ratio_per_sample <= threshold
        
        X_clean = X[mask]
        y_clean = y[mask]
        
        removed_count = np.sum(~mask)
        logger.info(f"ç­–ç•¥1: åˆ é™¤äº†{removed_count}ä¸ªæ ·æœ¬ (ç¼ºå¤±ç‡>{threshold*100:.1f}%)")
        logger.info(f"å‰©ä½™æ ·æœ¬æ•°: {X_clean.shape[0]} / {X.shape[0]}")
        
        return X_clean, y_clean
    
    def strategy_2_remove_features(self, X: np.ndarray, y: np.ndarray,
                                  threshold: float = 0.7,
                                  feature_names: Optional[List[str]] = None) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        ç­–ç•¥2: åˆ é™¤å«æœ‰è¿‡å¤šNaNçš„ç‰¹å¾
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            y: ç›®æ ‡å€¼
            threshold: ç¼ºå¤±ç‡é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤æ¯”ä¾‹çš„ç‰¹å¾å°†è¢«åˆ é™¤
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            
        Returns:
            æ¸…ç†åçš„X, y, ä¿ç•™çš„ç‰¹å¾åç§°
        """
        if feature_names is None:
            feature_names = [f'feature_{i}' for i in range(X.shape[1])]
            
        nan_ratio_per_feature = np.isnan(X).mean(axis=0)
        mask = nan_ratio_per_feature <= threshold
        
        X_clean = X[:, mask]
        remaining_features = [name for name, keep in zip(feature_names, mask) if keep]
        
        removed_count = np.sum(~mask)
        logger.info(f"ç­–ç•¥2: åˆ é™¤äº†{removed_count}ä¸ªç‰¹å¾ (ç¼ºå¤±ç‡>{threshold*100:.1f}%)")
        logger.info(f"å‰©ä½™ç‰¹å¾æ•°: {X_clean.shape[1]} / {X.shape[1]}")
        
        return X_clean, y, remaining_features
    
    def strategy_3_simple_imputation(self, X: np.ndarray, y: np.ndarray,
                                   strategy: str = 'mean') -> Tuple[np.ndarray, np.ndarray]:
        """
        ç­–ç•¥3: ç®€å•æ’å€¼å¡«å……
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            y: ç›®æ ‡å€¼
            strategy: æ’å€¼ç­–ç•¥ ('mean', 'median', 'most_frequent', 'constant')
            
        Returns:
            å¡«å……åçš„X, y
        """
        # å¤„ç†è¾“å…¥ç‰¹å¾
        imputer_X = SimpleImputer(strategy=strategy)
        X_imputed = imputer_X.fit_transform(X)
        self.imputers['X_simple'] = imputer_X
        
        # å¤„ç†ç›®æ ‡å€¼
        if np.isnan(y).any():
            imputer_y = SimpleImputer(strategy=strategy if strategy != 'most_frequent' else 'mean')
            y_imputed = imputer_y.fit_transform(y.reshape(-1, 1) if y.ndim == 1 else y)
            if y.ndim == 1:
                y_imputed = y_imputed.ravel()
            self.imputers['y_simple'] = imputer_y
        else:
            y_imputed = y
        
        logger.info(f"ç­–ç•¥3: ä½¿ç”¨{strategy}ç­–ç•¥è¿›è¡Œç®€å•æ’å€¼å¡«å……")
        
        return X_imputed, y_imputed
    
    def strategy_4_knn_imputation(self, X: np.ndarray, y: np.ndarray,
                                 n_neighbors: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç­–ç•¥4: KNNæ’å€¼å¡«å……
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            y: ç›®æ ‡å€¼
            n_neighbors: KNNçš„é‚»å±…æ•°é‡
            
        Returns:
            å¡«å……åçš„X, y
        """
        # å¤„ç†è¾“å…¥ç‰¹å¾
        imputer_X = KNNImputer(n_neighbors=n_neighbors)
        X_imputed = imputer_X.fit_transform(X)
        self.imputers['X_knn'] = imputer_X
        
        # å¤„ç†ç›®æ ‡å€¼
        if np.isnan(y).any():
            imputer_y = KNNImputer(n_neighbors=n_neighbors)
            y_imputed = imputer_y.fit_transform(y.reshape(-1, 1) if y.ndim == 1 else y)
            if y.ndim == 1:
                y_imputed = y_imputed.ravel()
            self.imputers['y_knn'] = imputer_y
        else:
            y_imputed = y
        
        logger.info(f"ç­–ç•¥4: ä½¿ç”¨KNNæ’å€¼å¡«å…… (k={n_neighbors})")
        
        return X_imputed, y_imputed
    
    def strategy_5_iterative_imputation(self, X: np.ndarray, y: np.ndarray,
                                       max_iter: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç­–ç•¥5: è¿­ä»£æ’å€¼å¡«å…… (MICEç®—æ³•)
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            y: ç›®æ ‡å€¼
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
            
        Returns:
            å¡«å……åçš„X, y
        """
        # å¤„ç†è¾“å…¥ç‰¹å¾
        imputer_X = IterativeImputer(max_iter=max_iter, random_state=42)
        X_imputed = imputer_X.fit_transform(X)
        self.imputers['X_iterative'] = imputer_X
        
        # å¤„ç†ç›®æ ‡å€¼
        if np.isnan(y).any():
            imputer_y = IterativeImputer(max_iter=max_iter, random_state=42)
            y_imputed = imputer_y.fit_transform(y.reshape(-1, 1) if y.ndim == 1 else y)
            if y.ndim == 1:
                y_imputed = y_imputed.ravel()
            self.imputers['y_iterative'] = imputer_y
        else:
            y_imputed = y
        
        logger.info(f"ç­–ç•¥5: ä½¿ç”¨è¿­ä»£æ’å€¼å¡«å…… (æœ€å¤§è¿­ä»£{max_iter}æ¬¡)")
        
        return X_imputed, y_imputed
    
    def strategy_6_hybrid_approach(self, X: np.ndarray, y: np.ndarray,
                                  high_missing_threshold: float = 0.7,
                                  sample_missing_threshold: float = 0.5) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç­–ç•¥6: æ··åˆæ–¹æ³•
        1. åˆ é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾
        2. åˆ é™¤é«˜ç¼ºå¤±ç‡æ ·æœ¬
        3. å¯¹å‰©ä½™æ•°æ®è¿›è¡ŒKNNæ’å€¼
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            y: ç›®æ ‡å€¼
            high_missing_threshold: é«˜ç¼ºå¤±ç‡ç‰¹å¾é˜ˆå€¼
            sample_missing_threshold: é«˜ç¼ºå¤±ç‡æ ·æœ¬é˜ˆå€¼
            
        Returns:
            å¤„ç†åçš„X, y
        """
        logger.info("ç­–ç•¥6: æ··åˆæ–¹æ³•å¤„ç†")
        
        original_shape = X.shape
        
        # æ­¥éª¤1: åˆ é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾
        feature_nan_ratio = np.isnan(X).mean(axis=0)
        feature_mask = feature_nan_ratio <= high_missing_threshold
        X_step1 = X[:, feature_mask]
        
        removed_features = np.sum(~feature_mask)
        logger.info(f"  æ­¥éª¤1: åˆ é™¤{removed_features}ä¸ªé«˜ç¼ºå¤±ç‡ç‰¹å¾")
        
        # æ­¥éª¤2: åˆ é™¤é«˜ç¼ºå¤±ç‡æ ·æœ¬
        sample_nan_ratio = np.isnan(X_step1).sum(axis=1) / X_step1.shape[1]
        sample_mask = sample_nan_ratio <= sample_missing_threshold
        X_step2 = X_step1[sample_mask]
        y_step2 = y[sample_mask]
        
        removed_samples = np.sum(~sample_mask)
        logger.info(f"  æ­¥éª¤2: åˆ é™¤{removed_samples}ä¸ªé«˜ç¼ºå¤±ç‡æ ·æœ¬")
        
        # æ­¥éª¤3: KNNæ’å€¼å¡«å……å‰©ä½™ç¼ºå¤±å€¼
        if np.isnan(X_step2).any():
            X_final, y_final = self.strategy_4_knn_imputation(X_step2, y_step2)
            logger.info(f"  æ­¥éª¤3: å¯¹å‰©ä½™ç¼ºå¤±å€¼è¿›è¡ŒKNNæ’å€¼")
        else:
            X_final, y_final = X_step2, y_step2
            logger.info(f"  æ­¥éª¤3: æ— éœ€æ’å€¼ï¼Œæ•°æ®å·²å®Œæ•´")
        
        logger.info(f"æ··åˆæ–¹æ³•å®Œæˆ: {original_shape} -> {X_final.shape}")
        
        return X_final, y_final
    
    def compare_strategies(self, X: np.ndarray, y: np.ndarray,
                          strategies: Optional[List[str]] = None) -> Dict[str, Dict]:
        """
        æ¯”è¾ƒä¸åŒNaNå¤„ç†ç­–ç•¥çš„æ•ˆæœ
        
        Args:
            X: è¾“å…¥ç‰¹å¾
            y: ç›®æ ‡å€¼
            strategies: è¦æ¯”è¾ƒçš„ç­–ç•¥åˆ—è¡¨
            
        Returns:
            å„ç­–ç•¥çš„æ¯”è¾ƒç»“æœ
        """
        if strategies is None:
            strategies = ['remove_samples', 'simple_mean', 'knn', 'iterative', 'hybrid']
        
        results = {}
        
        for strategy in strategies:
            try:
                if strategy == 'remove_samples':
                    X_proc, y_proc = self.strategy_1_remove_samples(X.copy(), y.copy())
                elif strategy == 'simple_mean':
                    X_proc, y_proc = self.strategy_3_simple_imputation(X.copy(), y.copy(), 'mean')
                elif strategy == 'simple_median':
                    X_proc, y_proc = self.strategy_3_simple_imputation(X.copy(), y.copy(), 'median')
                elif strategy == 'knn':
                    X_proc, y_proc = self.strategy_4_knn_imputation(X.copy(), y.copy())
                elif strategy == 'iterative':
                    X_proc, y_proc = self.strategy_5_iterative_imputation(X.copy(), y.copy())
                elif strategy == 'hybrid':
                    X_proc, y_proc = self.strategy_6_hybrid_approach(X.copy(), y.copy())
                else:
                    continue
                
                # è®¡ç®—å¤„ç†åçš„ç»Ÿè®¡ä¿¡æ¯
                results[strategy] = {
                    'final_shape': X_proc.shape,
                    'samples_retained': X_proc.shape[0] / X.shape[0],
                    'features_retained': X_proc.shape[1] / X.shape[1],
                    'has_nan': np.isnan(X_proc).any() or np.isnan(y_proc).any(),
                    'data_mean': np.nanmean(X_proc),
                    'data_std': np.nanstd(X_proc)
                }
                
            except Exception as e:
                results[strategy] = {'error': str(e)}
        
        return results
    
    def recommend_strategy(self, analysis: Dict[str, Any]) -> str:
        """
        æ ¹æ®æ•°æ®åˆ†æç»“æœæ¨èæœ€ä½³ç­–ç•¥
        
        Args:
            analysis: NaNåˆ†æç»“æœ
            
        Returns:
            æ¨èçš„ç­–ç•¥åç§°
        """
        total_samples = analysis['total_samples']
        complete_samples = analysis['complete_samples']
        complete_ratio = complete_samples / total_samples
        
        high_missing_features = len(analysis['features_with_high_nan'])
        total_features = analysis['total_features']
        
        avg_nan_per_sample = analysis['avg_nan_per_sample']
        
        print("\nğŸ¤– ç­–ç•¥æ¨èåˆ†æ:")
        print(f"  å®Œæ•´æ ·æœ¬æ¯”ä¾‹: {complete_ratio*100:.1f}%")
        print(f"  é«˜ç¼ºå¤±ç‰¹å¾æ•°: {high_missing_features}/{total_features}")
        print(f"  å¹³å‡æ¯æ ·æœ¬ç¼ºå¤±ç‰¹å¾æ•°: {avg_nan_per_sample:.1f}")
        
        if complete_ratio >= 0.7:
            recommendation = "remove_samples"
            reason = "å®Œæ•´æ ·æœ¬æ¯”ä¾‹è¾ƒé«˜ï¼Œå»ºè®®åˆ é™¤å«ç¼ºå¤±å€¼çš„æ ·æœ¬"
        elif high_missing_features > total_features * 0.3:
            recommendation = "hybrid"
            reason = "é«˜ç¼ºå¤±ç‰¹å¾è¾ƒå¤šï¼Œå»ºè®®ä½¿ç”¨æ··åˆæ–¹æ³•"
        elif avg_nan_per_sample <= 2:
            recommendation = "knn"
            reason = "ç¼ºå¤±å€¼è¾ƒå°‘ä¸”åˆ†æ•£ï¼ŒKNNæ’å€¼æ•ˆæœè¾ƒå¥½"
        elif total_samples >= 1000:
            recommendation = "iterative"
            reason = "æ ·æœ¬é‡å……è¶³ï¼Œå¯ä½¿ç”¨è¿­ä»£æ’å€¼è·å¾—æ›´å¥½æ•ˆæœ"
        else:
            recommendation = "simple_mean"
            reason = "æ ·æœ¬é‡è¾ƒå°‘ï¼Œä½¿ç”¨ç®€å•å‡å€¼æ’å€¼è¾ƒä¸ºç¨³å¦¥"
        
        print(f"\nğŸ’¡ æ¨èç­–ç•¥: {recommendation}")
        print(f"   æ¨èç†ç”±: {reason}")
        
        return recommendation


def demonstrate_nan_handling():
    """æ¼”ç¤ºNaNå¤„ç†çš„å®Œæ•´æµç¨‹"""
    print("ğŸš€ NaNå€¼å¤„ç†æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå«æœ‰NaNçš„ç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples, n_features = 1000, 15
    
    # ç”ŸæˆåŸºç¡€æ•°æ®
    X_clean = np.random.randn(n_samples, n_features).astype(np.float32)
    y_clean = (X_clean[:, :3].sum(axis=1) + np.random.randn(n_samples) * 0.1).astype(np.float32)
    
    # äººä¸ºå¼•å…¥NaNå€¼
    X = X_clean.copy()
    y = y_clean.copy()
    
    # åœ¨Xä¸­éšæœºå¼•å…¥NaN (ä¸åŒç‰¹å¾æœ‰ä¸åŒçš„ç¼ºå¤±ç‡)
    for i in range(n_features):
        missing_rate = np.random.uniform(0.05, 0.4)  # 5%-40%çš„ç¼ºå¤±ç‡
        n_missing = int(n_samples * missing_rate)
        missing_indices = np.random.choice(n_samples, n_missing, replace=False)
        X[missing_indices, i] = np.nan
    
    # åœ¨yä¸­å¼•å…¥å°‘é‡NaN
    y_missing_indices = np.random.choice(n_samples, int(n_samples * 0.02), replace=False)
    y[y_missing_indices] = np.nan
    
    print(f"åˆ›å»ºäº†å«NaNçš„ç¤ºä¾‹æ•°æ®: X{X.shape}, y{y.shape}")
    
    # åˆ›å»ºNaNå¤„ç†å™¨
    handler = NaNHandler()
    
    # åˆ†æNaNæ¨¡å¼
    analysis = handler.analyze_nan_pattern(X, y)
    handler.print_nan_summary(analysis)
    
    # å¯è§†åŒ–NaNæ¨¡å¼
    handler.visualize_nan_pattern(X, save_path="nan_pattern_analysis.png")
    
    # è·å–æ¨èç­–ç•¥
    recommended = handler.recommend_strategy(analysis)
    
    # æ¯”è¾ƒä¸åŒç­–ç•¥
    print("\nğŸ“Š ç­–ç•¥æ¯”è¾ƒç»“æœ:")
    comparison = handler.compare_strategies(X, y)
    
    for strategy, result in comparison.items():
        if 'error' in result:
            print(f"  {strategy}: âŒ é”™è¯¯ - {result['error']}")
        else:
            print(f"  {strategy}:")
            print(f"    æœ€ç»ˆå½¢çŠ¶: {result['final_shape']}")
            print(f"    æ ·æœ¬ä¿ç•™ç‡: {result['samples_retained']*100:.1f}%")
            print(f"    ç‰¹å¾ä¿ç•™ç‡: {result['features_retained']*100:.1f}%")
            print(f"    ä»æœ‰NaN: {'æ˜¯' if result['has_nan'] else 'å¦'}")
    
    # åº”ç”¨æ¨èç­–ç•¥
    print(f"\nğŸ¯ åº”ç”¨æ¨èç­–ç•¥: {recommended}")
    
    if recommended == 'remove_samples':
        X_final, y_final = handler.strategy_1_remove_samples(X, y)
    elif recommended == 'simple_mean':
        X_final, y_final = handler.strategy_3_simple_imputation(X, y, 'mean')
    elif recommended == 'knn':
        X_final, y_final = handler.strategy_4_knn_imputation(X, y)
    elif recommended == 'iterative':
        X_final, y_final = handler.strategy_5_iterative_imputation(X, y)
    elif recommended == 'hybrid':
        X_final, y_final = handler.strategy_6_hybrid_approach(X, y)
    
    print(f"âœ… å¤„ç†å®Œæˆ!")
    print(f"   åŸå§‹æ•°æ®: X{X.shape}, y{y.shape}")
    print(f"   å¤„ç†å: X{X_final.shape}, y{y_final.shape}")
    print(f"   æ˜¯å¦è¿˜æœ‰NaN: {np.isnan(X_final).any() or np.isnan(y_final).any()}")
    
    return X_final, y_final, handler


def create_nan_data_template():
    """åˆ›å»ºå¤„ç†å«NaNæ•°æ®çš„æ¨¡æ¿ä»£ç """
    template = '''
# å¤„ç†å«NaNå€¼æ•°æ®çš„å®Œæ•´æ¨¡æ¿
import numpy as np
import yaml
from nan_handling_guide import NaNHandler
from data_processor import DataProcessor
from mlp_model import create_model_from_config
from trainer import MLPTrainer

def train_with_nan_data():
    """å¤„ç†å«NaNæ•°æ®å¹¶è®­ç»ƒæ¨¡å‹çš„å®Œæ•´æµç¨‹"""
    
    # 1. åŠ è½½æ‚¨çš„å«NaNæ•°æ®
    # ================================
    # TODO: æ›¿æ¢ä¸ºæ‚¨çš„å®é™…æ•°æ®åŠ è½½ä»£ç 
    X = your_data_with_nan  # å«NaNçš„è¾“å…¥ç‰¹å¾
    y = your_targets_with_nan  # å¯èƒ½å«NaNçš„ç›®æ ‡å€¼
    # ================================
    
    # 2. åˆ›å»ºNaNå¤„ç†å™¨å¹¶åˆ†ææ•°æ®
    handler = NaNHandler()
    analysis = handler.analyze_nan_pattern(X, y)
    handler.print_nan_summary(analysis)
    
    # 3. è·å–æ¨èç­–ç•¥
    recommended_strategy = handler.recommend_strategy(analysis)
    
    # 4. åº”ç”¨å¤„ç†ç­–ç•¥
    if recommended_strategy == 'remove_samples':
        X_clean, y_clean = handler.strategy_1_remove_samples(X, y, threshold=0.5)
    elif recommended_strategy == 'simple_mean':
        X_clean, y_clean = handler.strategy_3_simple_imputation(X, y, 'mean')
    elif recommended_strategy == 'knn':
        X_clean, y_clean = handler.strategy_4_knn_imputation(X, y, n_neighbors=5)
    elif recommended_strategy == 'iterative':
        X_clean, y_clean = handler.strategy_5_iterative_imputation(X, y)
    elif recommended_strategy == 'hybrid':
        X_clean, y_clean = handler.strategy_6_hybrid_approach(X, y)
    else:
        # é»˜è®¤ä½¿ç”¨æ··åˆæ–¹æ³•
        X_clean, y_clean = handler.strategy_6_hybrid_approach(X, y)
    
    print(f"NaNå¤„ç†å®Œæˆ: {X.shape} -> {X_clean.shape}")
    
    # 5. éªŒè¯æ•°æ®æ¸…æ´åº¦
    assert not np.isnan(X_clean).any(), "è¾“å…¥ç‰¹å¾ä»æœ‰NaNå€¼"
    assert not np.isnan(y_clean).any(), "ç›®æ ‡å€¼ä»æœ‰NaNå€¼"
    print("âœ… æ•°æ®éªŒè¯é€šè¿‡ï¼Œæ— NaNå€¼")
    
    # 6. æ­£å¸¸çš„æ¨¡å‹è®­ç»ƒæµç¨‹
    config = yaml.safe_load(open('config.yaml', 'r', encoding='utf-8'))
    
    processor = DataProcessor(config)
    processor.load_data_from_arrays(X_clean, y_clean)
    processor.normalize_data()
    
    X_train, X_val, X_test, y_train, y_val, y_test = processor.split_data()
    train_loader, val_loader, test_loader = processor.create_data_loaders(
        X_train, X_val, X_test, y_train, y_val, y_test
    )
    
    model = create_model_from_config(config, processor.input_dim, processor.output_dim)
    trainer = MLPTrainer(model, config)
    
    print("å¼€å§‹è®­ç»ƒ...")
    history = trainer.train(train_loader, val_loader)
    
    # 7. è¯„ä¼°æ¨¡å‹
    from evaluator import ModelEvaluator
    evaluator = ModelEvaluator()
    test_pred, test_true = trainer.predict(test_loader)
    test_pred_orig = processor.inverse_transform_predictions(test_pred)
    test_true_orig = processor.inverse_transform_predictions(test_true)
    
    metrics = evaluator.evaluate_model(test_true_orig, test_pred_orig)
    print(f"è®­ç»ƒå®Œæˆï¼RÂ²åˆ†æ•°: {metrics['r2']:.4f}")
    
    return trainer, processor, metrics, handler

if __name__ == "__main__":
    train_with_nan_data()
'''
    
    with open('nan_data_template.py', 'w', encoding='utf-8') as f:
        f.write(template)
    
    print("ğŸ“ å·²åˆ›å»ºNaNæ•°æ®å¤„ç†æ¨¡æ¿: nan_data_template.py")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demonstrate_nan_handling()
    
    # åˆ›å»ºæ¨¡æ¿
    create_nan_data_template()
    
    print("\nğŸ‰ NaNå¤„ç†æŒ‡å—æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. é¦–å…ˆåˆ†ææ‚¨çš„æ•°æ®NaNåˆ†å¸ƒæ¨¡å¼")
    print("2. æ ¹æ®æ¨èé€‰æ‹©åˆé€‚çš„å¤„ç†ç­–ç•¥")
    print("3. éªŒè¯å¤„ç†åçš„æ•°æ®è´¨é‡")
    print("4. ä½¿ç”¨å¤„ç†åçš„æ•°æ®è¿›è¡Œæ­£å¸¸è®­ç»ƒ")