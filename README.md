# MLPæ•°å€¼é¢„æµ‹æ¨¡å‹

ä¸€ä¸ªåŸºäºPyTorchçš„å¤šå±‚æ„ŸçŸ¥æœº(MLP)æ•°å€¼é¢„æµ‹ç³»ç»Ÿï¼Œä¸“ä¸ºå¤„ç†nä¸ªæµ®ç‚¹æ•°è¾“å…¥é¢„æµ‹mä¸ªæµ®ç‚¹æ•°è¾“å‡ºçš„å›å½’ä»»åŠ¡è€Œè®¾è®¡ã€‚

## é¡¹ç›®ç‰¹ç‚¹

- ğŸš€ **é«˜æ€§èƒ½**: åŸºäºPyTorchå®ç°ï¼Œæ”¯æŒGPUåŠ é€Ÿ
- ğŸ”§ **é«˜åº¦å¯é…ç½®**: é€šè¿‡YAMLé…ç½®æ–‡ä»¶è½»æ¾è°ƒæ•´æ¨¡å‹å‚æ•°
- ğŸ“Š **å…¨é¢è¯„ä¼°**: æä¾›å¤šç§è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–åŠŸèƒ½
- ğŸ›¡ï¸ **ç¨³å®šè®­ç»ƒ**: é›†æˆæ—©åœã€å­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ªç­‰æŠ€æœ¯
- ğŸ“ˆ **è¯¦ç»†ç›‘æ§**: å®Œæ•´çš„è®­ç»ƒå†å²è®°å½•å’Œæ—¥å¿—ç³»ç»Ÿ
- ğŸ”„ **æ˜“äºä½¿ç”¨**: æä¾›ç®€æ´çš„APIå’Œä¸°å¯Œçš„ä½¿ç”¨ç¤ºä¾‹

## é¡¹ç›®ç»“æ„

```
mlp/
â”œâ”€â”€ config.yaml           # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt       # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ main.py               # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ example_usage.py      # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ data_processor.py     # æ•°æ®å¤„ç†æ¨¡å—
â”œâ”€â”€ mlp_model.py          # MLPæ¨¡å‹å®šä¹‰
â”œâ”€â”€ trainer.py            # æ¨¡å‹è®­ç»ƒå™¨
â”œâ”€â”€ evaluator.py          # æ¨¡å‹è¯„ä¼°å™¨
â”œâ”€â”€ README.md             # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ models/               # æ¨¡å‹ä¿å­˜ç›®å½•
â”œâ”€â”€ logs/                 # æ—¥å¿—æ–‡ä»¶ç›®å½•
â””â”€â”€ plots/                # å›¾è¡¨ä¿å­˜ç›®å½•
```

## å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
import numpy as np
from main import load_config, prepare_data, train_model, evaluate_model

# åŠ è½½é…ç½®
config = load_config('config.yaml')

# å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼‰
processor, data_loaders = prepare_data(config, data_source="sample")

# è®­ç»ƒæ¨¡å‹
trainer = train_model(config, processor, data_loaders)

# è¯„ä¼°æ¨¡å‹
results = evaluate_model(trainer, processor, data_loaders)
```

### 2. ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®

```python
import numpy as np
from data_processor import DataProcessor
from mlp_model import create_model_from_config
from trainer import MLPTrainer

# å‡†å¤‡ä½ çš„æ•°æ®
X = np.random.randn(1000, 10).astype(np.float32)  # 1000ä¸ªæ ·æœ¬ï¼Œ10ä¸ªç‰¹å¾
y = np.random.randn(1000, 3).astype(np.float32)   # 1000ä¸ªæ ·æœ¬ï¼Œ3ä¸ªç›®æ ‡

# æ•°æ®å¤„ç†
processor = DataProcessor(config)
processor.load_data_from_arrays(X, y)
processor.normalize_data()

# åˆ†å‰²æ•°æ®
X_train, X_val, X_test, y_train, y_val, y_test = processor.split_data()
train_loader, val_loader, test_loader = processor.create_data_loaders(
    X_train, X_val, X_test, y_train, y_val, y_test
)

# åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
model = create_model_from_config(config, processor.input_dim, processor.output_dim)
trainer = MLPTrainer(model, config)
history = trainer.train(train_loader, val_loader)
```

### 3. å‘½ä»¤è¡Œä½¿ç”¨

```bash
# è®­ç»ƒæ¨¡å‹
python main.py --mode train --config config.yaml --save_dir models

# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
python main.py --mode predict --model_path models/final_model.pth --config config.yaml
```

### 4. è¿è¡Œç¤ºä¾‹

```bash
python example_usage.py
```

## é…ç½®è¯´æ˜

ä¸»è¦é…ç½®å‚æ•°è¯´æ˜ï¼š

```yaml
# æ•°æ®é…ç½®
data:
  train_ratio: 0.8        # è®­ç»ƒé›†æ¯”ä¾‹
  val_ratio: 0.1          # éªŒè¯é›†æ¯”ä¾‹
  test_ratio: 0.1         # æµ‹è¯•é›†æ¯”ä¾‹
  normalize: true         # æ˜¯å¦æ ‡å‡†åŒ–æ•°æ®

# æ¨¡å‹é…ç½®
model:
  hidden_layers: [128, 64, 32]  # éšè—å±‚ç¥ç»å…ƒæ•°é‡
  dropout_rate: 0.2             # Dropoutæ¯”ç‡
  activation: "relu"            # æ¿€æ´»å‡½æ•°

# è®­ç»ƒé…ç½®
training:
  batch_size: 32               # æ‰¹æ¬¡å¤§å°
  epochs: 100                  # è®­ç»ƒè½®æ•°
  learning_rate: 0.001         # å­¦ä¹ ç‡
  early_stopping_patience: 10  # æ—©åœè€å¿ƒå€¼
```

## æ¨¡å‹ç‰¹æ€§

### æ”¯æŒçš„æ¿€æ´»å‡½æ•°
- ReLU
- Tanh
- Sigmoid
- Leaky ReLU

### æ”¯æŒçš„ä¼˜åŒ–å™¨
- Adam
- SGD
- RMSprop

### æ”¯æŒçš„æŸå¤±å‡½æ•°
- MSE (å‡æ–¹è¯¯å·®)
- MAE (å¹³å‡ç»å¯¹è¯¯å·®)
- Huber Loss

### è¯„ä¼°æŒ‡æ ‡
- MSE (å‡æ–¹è¯¯å·®)
- RMSE (å‡æ–¹æ ¹è¯¯å·®)
- MAE (å¹³å‡ç»å¯¹è¯¯å·®)
- RÂ² (å†³å®šç³»æ•°)
- MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®)
- SMAPE (å¯¹ç§°å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®)
- çš®å°”é€Šç›¸å…³ç³»æ•°

## APIå‚è€ƒ

### DataProcessorç±»

```python
# åˆ›å»ºæ•°æ®å¤„ç†å™¨
processor = DataProcessor(config)

# ä»æ•°ç»„åŠ è½½æ•°æ®
processor.load_data_from_arrays(X, y)

# ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®
processor.load_data_from_csv('data.csv', target_columns=['target1', 'target2'])

# æ•°æ®æ ‡å‡†åŒ–
processor.normalize_data()

# æ•°æ®åˆ†å‰²
X_train, X_val, X_test, y_train, y_val, y_test = processor.split_data()
```

### MLPModelç±»

```python
# åˆ›å»ºæ¨¡å‹
model = MLPModel(
    input_dim=10,
    output_dim=3,
    hidden_layers=[128, 64, 32],
    activation='relu',
    dropout_rate=0.2
)

# è·å–æ¨¡å‹ä¿¡æ¯
info = model.get_model_info()
```

### MLPTrainerç±»

```python
# åˆ›å»ºè®­ç»ƒå™¨
trainer = MLPTrainer(model, config)

# è®­ç»ƒæ¨¡å‹
history = trainer.train(train_loader, val_loader)

# è¿›è¡Œé¢„æµ‹
predictions, targets = trainer.predict(test_loader)

# å•æ ·æœ¬é¢„æµ‹
pred = trainer.predict_single(x_new)
```

### ModelEvaluatorç±»

```python
# åˆ›å»ºè¯„ä¼°å™¨
evaluator = ModelEvaluator(save_plots=True)

# è¯„ä¼°æ¨¡å‹
metrics = evaluator.evaluate_model(y_true, y_pred, "test")

# ç»˜åˆ¶å›¾è¡¨
evaluator.plot_predictions_vs_actual(y_true, y_pred)
evaluator.plot_residuals(y_true, y_pred)
evaluator.plot_training_history(history)
```

## é«˜çº§åŠŸèƒ½

### æ—©åœæœºåˆ¶
è‡ªåŠ¨ç›‘æ§éªŒè¯æŸå¤±ï¼Œåœ¨æ€§èƒ½ä¸å†æå‡æ—¶åœæ­¢è®­ç»ƒï¼š

```python
early_stopping = EarlyStopping(
    patience=10,
    min_delta=1e-6,
    restore_best_weights=True
)
```

### æ¨¡å‹æ£€æŸ¥ç‚¹
è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ï¼š

```python
checkpoint = ModelCheckpoint(
    filepath='best_model.pth',
    monitor='val_loss',
    save_best_only=True
)
```

### å­¦ä¹ ç‡è°ƒåº¦
è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼š

```python
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=5
)
```

## æ³¨æ„äº‹é¡¹

1. **ç¦»çº¿ç¯å¢ƒ**: é¡¹ç›®è®¾è®¡ä¸ºåœ¨ç¦»çº¿ç¯å¢ƒä¸­è¿è¡Œï¼Œä¸ä¾èµ–åœ¨çº¿æœåŠ¡
2. **æ•°æ®æ ¼å¼**: è¾“å…¥æ•°æ®åº”ä¸ºnumpyæ•°ç»„ï¼Œdtypeä¸ºfloat32
3. **å†…å­˜ä½¿ç”¨**: å¤§æ•°æ®é›†å»ºè®®è°ƒæ•´batch_sizeä»¥æ§åˆ¶å†…å­˜ä½¿ç”¨
4. **GPUæ”¯æŒ**: è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨å¯ç”¨çš„GPUè¿›è¡Œè®­ç»ƒ

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**: å‡å°batch_sizeæˆ–hidden_layerså¤§å°
2. **è®­ç»ƒä¸æ”¶æ•›**: è°ƒæ•´å­¦ä¹ ç‡æˆ–å¢åŠ è®­ç»ƒè½®æ•°
3. **è¿‡æ‹Ÿåˆ**: å¢åŠ dropout_rateæˆ–å‡å°‘æ¨¡å‹å¤æ‚åº¦
4. **æ¬ æ‹Ÿåˆ**: å¢åŠ æ¨¡å‹å¤æ‚åº¦æˆ–å‡å°‘æ­£åˆ™åŒ–

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ
2. åˆç†è®¾ç½®batch_sizeï¼ˆé€šå¸¸32-128ï¼‰
3. ä½¿ç”¨æ•°æ®æ ‡å‡†åŒ–
4. å¯ç”¨æ—©åœæœºåˆ¶é¿å…è¿‡æ‹Ÿåˆ
5. ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–æ”¶æ•›

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›é¡¹ç›®ã€‚

## æ›´æ–°æ—¥å¿—

### v1.0.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒåŸºæœ¬çš„MLPè®­ç»ƒå’Œé¢„æµ‹åŠŸèƒ½
- å®Œæ•´çš„æ•°æ®å¤„ç†å’Œè¯„ä¼°ç³»ç»Ÿ
- ä¸°å¯Œçš„é…ç½®é€‰é¡¹å’Œä½¿ç”¨ç¤ºä¾‹